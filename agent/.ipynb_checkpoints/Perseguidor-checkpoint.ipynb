{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cbc20b",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8830d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 15:06:29.270067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "import json\n",
    "import socket\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "# Importando Address do socket UDP\n",
    "UDP_IP = \"0.0.0.0\"\n",
    "UDP_PORT = 2223\n",
    "addrinfo = socket.getaddrinfo(\n",
    "  UDP_IP, UDP_PORT,\n",
    "  socket.AF_INET, socket.SOCK_DGRAM)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c288f",
   "metadata": {},
   "source": [
    "## Criação do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7300fc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criação do ambiente \n",
    "class PerseguidorEnv():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state_space = np.array(np.zeros(13))\n",
    "        self.action_space = np.array(np.zeros(4))\n",
    "        self.width = 1333\n",
    "        self.height = 735\n",
    "        self.high_x = 1333\n",
    "        self.low_x = 35\n",
    "        self.high_y = 735\n",
    "        self.low_y = 114\n",
    "        self.seconds_done = 60\n",
    "        self.score_done = 5\n",
    "        self.max_distance = 1000\n",
    "\n",
    "        \n",
    "    # carrega observações do ambiente\n",
    "    def _get_states(self):\n",
    "        # carrega dados recebidas do servidor\n",
    "        info = self._get_info()\n",
    "        \n",
    "        self._direction = np.array([\n",
    "            (1 if info['player']['atual_action']==0 else 0), # direction atual\n",
    "            (1 if info['player']['atual_action']==1 else 0), # direction atual\n",
    "            (1 if info['player']['atual_action']==2 else 0), # direction atual\n",
    "            (1 if info['player']['atual_action']==3 else 0), # direction atual\n",
    "        ])\n",
    "        \n",
    "        self._location = np.array(np.zeros(4))\n",
    "        self._danger = np.array(np.zeros(4))\n",
    "        self._walls = np.array(np.zeros(4))\n",
    "        \n",
    "        direct = ['upper','right','bottom','left']\n",
    "        for i in range(len(direct)):\n",
    "            if info['player']['danger_{}'.format(direct[i])]:\n",
    "                self._danger[i] = 1\n",
    "            if info['player']['wall_{}'.format(direct[i])]:\n",
    "                self._walls[i] = 1\n",
    "            self._location[i] = info['player']['location_{}'.format(direct[i])]\n",
    "        \n",
    "        self._coins = np.array(np.zeros(4))\n",
    "        if (info[\"coins\"][0][\"has\"]):\n",
    "            self._coins[0] = info['coins'][0]['y']< info['player']['y']  # coin up\n",
    "            self._coins[1] = info['coins'][0]['x'] > info['player']['x']  # coin right\n",
    "            self._coins[2] = info['coins'][0]['y'] > info['player']['y']  # coin down\n",
    "            self._coins[3] = info['coins'][0]['x'] < info['player']['x']  # coin left\n",
    "        if (info[\"coins\"][1][\"has\"]):\n",
    "            self._coins[0] = info['coins'][1]['y']< info['player']['y'] if self._coins[0] == 0 else 1  # coin up\n",
    "            self._coins[1] = info['coins'][1]['x'] > info['player']['x'] if self._coins[1] == 0 else 1  # coin right\n",
    "            self._coins[2] = info['coins'][1]['y'] > info['player']['y'] if self._coins[2] == 0 else 1  # coin down\n",
    "            self._coins[3] = info['coins'][1]['x'] < info['player']['x'] if self._coins[3] == 0 else 1  # coin left\n",
    "       \n",
    "        # define array game\n",
    "        self._game = np.array([\n",
    "            #info['player']['wall'], # wall\n",
    "            int(info['player']['distance']), # danger distance\n",
    "            #info[\"player\"]['seconds'],\n",
    "            #info[\"player\"]['score'],\n",
    "        ])\n",
    "        \n",
    "        i_d = int(info['player']['distance']) \n",
    "        self._distance = np.array([\n",
    "            1 if i_d < 400 and i_d >= 300 else 0,\n",
    "            1 if i_d < 300 and i_d >= 200 else 0,\n",
    "            1 if i_d < 200 and i_d >= 100 else 0,\n",
    "            1 if i_d < 100 and i_d >= 0 else 0,\n",
    "        ])\n",
    "        \n",
    "        self._persons = np.array([\n",
    "            int(info['player']['x']),\n",
    "            int(info['player']['y']),\n",
    "            int(info['enemy']['x']),\n",
    "            int(info['enemy']['y']),\n",
    "        ])\n",
    "        \n",
    "        states = np.concatenate((self._danger, self._direction, self._walls, self._game), axis=None)\n",
    "        states = np.array(states)\n",
    "        # retorno\n",
    "        return states, info\n",
    "    \n",
    "    # carrega dados do servidor\n",
    "    def _get_info(self):\n",
    "        with socket.socket(*addrinfo[:3]) as sock:\n",
    "            sock.connect(addrinfo[4])\n",
    "            # pecorre ate receber socket do servidor\n",
    "            while True:\n",
    "                # envio de socket para servidor\n",
    "                sock.sendto(str.encode(''), (UDP_IP, UDP_PORT))\n",
    "                # define dados recebidos\n",
    "                data, addr = sock.recvfrom(1024)\n",
    "                # se existir dados\n",
    "                if data:\n",
    "                    # conversao de json para python\n",
    "                    res = json.loads(data)\n",
    "                    # retorno\n",
    "                    return res['game']\n",
    "    \n",
    "    # recarrega dados do servidor e informaçõs do ambiente\n",
    "    def reset(self): \n",
    "        #define observação do ambiente e dados do servidor\n",
    "        obs, info = self._get_states() \n",
    "        \n",
    "        # retorno\n",
    "        return obs, info\n",
    "    \n",
    "    def _restart_game(self):\n",
    "        agent_step = str.encode(json.dumps({'action': -1,'restart': True, 'socket': 'agent'}))\n",
    "        with socket.socket(*addrinfo[:3]) as sock:\n",
    "            sock.connect(addrinfo[4])\n",
    "            sock.sendto(agent_step, (UDP_IP, UDP_PORT))\n",
    "\n",
    "    # envia para o servidor socket ação recebida e carrega observações do ambiente recente\n",
    "    def step(self, action):\n",
    "        # conversão da ação recebida para enviar no servidor socket\n",
    "        agent_step = str.encode(json.dumps({'action': int(action),'restart': False, 'socket': 'agent'}))\n",
    "        # envia para servidor socket a ação recebida e preditada.\n",
    "        with socket.socket(*addrinfo[:3]) as sock:\n",
    "            sock.connect(addrinfo[4])\n",
    "            sock.sendto(agent_step, (UDP_IP, UDP_PORT))\n",
    "        \n",
    "        #define observação do ambiente e dados do servidor\n",
    "        obs, info = self._get_states()\n",
    "        done, reward = False, 0\n",
    "           \n",
    "      \n",
    "        #if info['player']['seconds'] >= self.seconds_done and info['player']['score'] >= self.score_done:\n",
    "        #    done=True \n",
    "        #    self._restart_game()\n",
    "            \n",
    "        #if (info['coins'][0]['has'] and info['coins'][0]['distance']<90) or (info['coins'][1]['colision'] and info['coins'][1]['distance']<90):\n",
    "        #    reward = 200\n",
    "        #else:\n",
    "        calc_distance = (info['player']['distance'] / self.max_distance)\n",
    "        # se a velocidade for 0 e se a distancia for menor que 70\n",
    "        if (info['player']['distance'] < 300):\n",
    "            if (info['player']['colision']):\n",
    "                reward = -100\n",
    "                done = True\n",
    "                self._restart_game()\n",
    "            else:\n",
    "                reward = -100 * (1-calc_distance)\n",
    "        #se não\n",
    "        else:\n",
    "            reward = int(calc_distance * 15)\n",
    "                        \n",
    "        # retorno\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    # função destinada para iniciar o server socket e o jogo\n",
    "    def start(self):\n",
    "        # iniciar processo do servidor socket\n",
    "        server = subprocess.Popen(['node ~/posgraduacao/tcc/projeto/server/index.js'], shell=True)\n",
    "        # aguardar 1 segundo\n",
    "        time.sleep(1)\n",
    "        # iniciar processo do jogo\n",
    "        game = subprocess.Popen(['love ~/posgraduacao/tcc/projeto/game'], shell=True)\n",
    "\n",
    "    # função destinada para parar o server socket e o jogo\n",
    "    def stop(self):\n",
    "        try:\n",
    "            # check se existe o processo do jogo\n",
    "            pid = list(map(int,subprocess.check_output([\"pidof\",'love']).split()))[0]\n",
    "            # condição de existe\n",
    "            if pid != 0:\n",
    "                # encerra o processo do jogo.\n",
    "                os.system('kill '+str(pid))\n",
    "        except:\n",
    "            pid = 0\n",
    "            \n",
    "        # aguardar 1 segundo\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            # check se existe o processo do servidor socket\n",
    "            pid = list(map(int,subprocess.check_output([\"pidof\",'node']).split()))[0]\n",
    "            # condição de existe\n",
    "            if pid != 0:\n",
    "                # encerra o processo do servidor socket\n",
    "                os.system('kill '+str(pid))\n",
    "        except:\n",
    "            pid = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e1f81",
   "metadata": {},
   "source": [
    "## Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990d782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(name, data):\n",
    "    with open(name, 'wb') as filehandle:\n",
    "        pickle.dump(data, filehandle)\n",
    "        filehandle.close()\n",
    "        \n",
    "def read_list(name):\n",
    "    data = []\n",
    "    with open(name, 'rb') as filehandle:\n",
    "        data = pickle.load(filehandle)\n",
    "        filehandle.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c47db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hitory_list(algorithm, load_model):\n",
    "    history = dict()\n",
    "    history['ep_reward'] = []\n",
    "    history['avg_reward'] = []\n",
    "    history['obs_history'] = []\n",
    "    history['action_history'] = []\n",
    "    history['reward_history'] = []\n",
    "    history['loss_history'] = []\n",
    "    history['memory_history'] = []\n",
    "    \n",
    "    for l in history:\n",
    "        filename = f'{algorithm}/{l}.dat'\n",
    "        if load_model:\n",
    "            history[l] = read_list(filename)\n",
    "            #history[l] = np.loadtxt(filename, dtype=float)\n",
    "        else:\n",
    "            if(os.path.isfile(filename)):\n",
    "                os.remove(filename)\n",
    "                \n",
    "    return history\n",
    "\n",
    "def set_history_list(algorithm,history):\n",
    "    # Save lists\n",
    "    for l in history:\n",
    "        filename = f'{algorithm}/{l}.dat'\n",
    "        #np.savetxt(filename, np.array(history[l]))\n",
    "        write_list(filename,history[l])\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918c8e4",
   "metadata": {},
   "source": [
    "## Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeab01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, params, load_model=False):\n",
    "        self.action_space = params['action_space']\n",
    "        self.state_space = params['state_space']\n",
    "        self.epsilon = params['epsilon'] \n",
    "        self.gamma = params['gamma'] \n",
    "        self.batch_size = params['batch_size'] \n",
    "        self.epsilon_min = params['epsilon_min'] \n",
    "        self.epsilon_decay = params['epsilon_decay'] \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.layer_sizes = params['layer_sizes']\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        if load_model:\n",
    "            self.model = self.load_model()\n",
    "        else:\n",
    "            self.model = self.build_model()\n",
    "            \n",
    "        if (len(history['memory_history'])>0):\n",
    "            for i in range(len(history['memory_history'])):\n",
    "                self.memory.append(history['memory_history'][i])\n",
    "                \n",
    "        print('load memory len -> ({})'.format(len(history['memory_history'])))\n",
    "    \n",
    "    def compile_model(self, model):\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        model = keras.models.load_model('dqn/perseguidor.tf')\n",
    "        model.load_weights('dqn/perseguidor.h5')\n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        try:\n",
    "            os.remove(\"dqn/perseguidor.tf\")\n",
    "            os.remove(\"dqn/perseguidor.h5\")\n",
    "        except:\n",
    "            print(\"Não existe arquivos e diretorio para exclusão\")\n",
    "        \n",
    "        model = Sequential()\n",
    "        for i in range(len(self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                model.add(Dense(self.layer_sizes[i], input_shape=(self.state_space,), activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(self.layer_sizes[i], activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        return self.compile_model(model)\n",
    "        \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        history['memory_history'].append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        #targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1-dones)\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1))\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        loss = self.model.fit(states, targets_full, epochs=1, verbose=2)\n",
    "        history['loss_history'].append(loss.history['loss'][0])   \n",
    "\n",
    "        #loss = self.model.train_on_batch(states, targets_full)\n",
    "        #history['loss_history'].append(loss)   \n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        #self.memory.clear()\n",
    "        \n",
    "def train_dqn(agent, env, params):\n",
    "    for e in range(params['episode']):\n",
    "        state, info = env.reset()\n",
    "        state = np.reshape(state, (1, params['state_space']))\n",
    "        done, score = False, 0\n",
    "        \n",
    "        for s in range(100):\n",
    "            gamestate = env._get_info()['info']['gamestate']\n",
    "            if gamestate==1:\n",
    "                action = agent.act(state)\n",
    "                history['action_history'].append(action)\n",
    "\n",
    "                prev_state = state\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                history['obs_history'].append(next_state)\n",
    "                history['reward_history'].append(reward)\n",
    "                score += reward\n",
    "\n",
    "                next_state = np.reshape(next_state, (1, params['state_space']))\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "                if params['batch_size'] > 1:\n",
    "                    agent.replay()\n",
    "                \n",
    "                print(\"Episode==>{}, Step==>{}, Reward==>{}, Action ==> {}, State == {}\".format(e,s, reward, action,0))\n",
    "                if done:\n",
    "                    print(f'final state before dying: {str(prev_state)}')\n",
    "                    print(f'episode: {e+1}/{e}, score: {score}')\n",
    "                    break\n",
    "            else:\n",
    "                while (gamestate!=1):\n",
    "                    gamestate = env._get_info()['info']['gamestate']\n",
    "                \n",
    "                \n",
    "                \n",
    "        history['ep_reward'].append(score)\n",
    "        avg_reward = np.mean(history['ep_reward'][-40:])\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(e, avg_reward))\n",
    "        history['avg_reward'].append(avg_reward)\n",
    "        \n",
    "    \n",
    "\n",
    "def start_dqn(params, env, load_model=False):\n",
    "    try:\n",
    "        agent = DQN(params, load_model)\n",
    "         # Iniciar Server Socket e Game Love lua\n",
    "        env.start()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        params['result'] = train_dqn(agent, env, params)\n",
    "\n",
    "    finally:\n",
    "        # Fechar Server Socket e Game Love lua\n",
    "        env.stop()\n",
    "        \n",
    "        # Save the models\n",
    "        agent.model.save('dqn/perseguidor.tf')\n",
    "        agent.model.save_weights('dqn/perseguidor.h5')\n",
    "        \n",
    "        # Save the lists\n",
    "        set_history_list('dqn',history)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda469d",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c015339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "            \n",
    "            \n",
    "def get_actor(params):\n",
    "        # Initialize weights between -3e-3 and 3-e3\n",
    "        last_init = tf.random_uniform_initializer(minval=0, maxval=1)  # kernel_initializer=last_init\n",
    "        inputs = layers.Input(shape=(params['state_space'],))\n",
    "        out = layers.Dense(params['layer_sizes'][1], activation=\"relu\")(inputs)\n",
    "        out = layers.Dense(params['layer_sizes'][2], activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(params['action_space'], activation=\"tanh\")(out)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model\n",
    "\n",
    "def get_critic(params):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(params['state_space']))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(params['action_space']))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(params['layer_sizes'][1], activation=\"relu\")(concat)\n",
    "    out = layers.Dense(params['layer_sizes'][2], activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(params['action_space'])(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, params, load_model=False, buffer_capacity=50000, batch_size=500, ):\n",
    "        \n",
    "        self.num_states = params['state_space']\n",
    "        self.num_actions = params['action_space'] #params['action_space']\n",
    "        self.upper_bound = params['upper_bound']\n",
    "        self.lower_bound = params['lower_bound']\n",
    "        self.gamma = params['gamma']\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(params['learning_rate'])\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(params['learning_rate'])\n",
    "        self.actor_model = []\n",
    "        self.critic_model = []\n",
    "        self.target_actor = []\n",
    "        self.target_critic = []\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        if load_model:\n",
    "            self.actor_model = tf.keras.models.load_model('ddpg/perseguidor_actor')\n",
    "            self.critic_model = tf.keras.models.load_model('ddpg/perseguidor_critic')\n",
    "            self.target_actor = tf.keras.models.load_model('ddpg/perseguidor_target_actor')\n",
    "            self.target_critic = tf.keras.models.load_model('ddpg/perseguidor_target_critic')\n",
    "\n",
    "            self.actor_model.load_weights(\"ddpg/perseguidor_actor.h5\")\n",
    "            self.critic_model.load_weights(\"ddpg/perseguidor_critic.h5\")\n",
    "            self.target_actor.load_weights(\"ddpg/perseguidor_target_actor.h5\")\n",
    "            self.target_critic.load_weights(\"ddpg/perseguidor_target_critic.h5\")\n",
    "        else:\n",
    "            try:\n",
    "                shutil.rmtree(\"ddpg/perseguidor_actor\")\n",
    "                shutil.rmtree(\"ddpg/perseguidor_critic\")\n",
    "                shutil.rmtree(\"ddpg/perseguidor_target_actor\")\n",
    "                shutil.rmtree(\"ddpg/perseguidor_target_critic\")\n",
    "\n",
    "                os.remove(\"ddpg/perseguidor_actor.h5\")\n",
    "                os.remove(\"ddpg/perseguidor_critic.h5\")\n",
    "                os.remove(\"ddpg/perseguidor_target_actor.h5\")\n",
    "                os.remove(\"ddpg/perseguidor_target_critic.h5\")\n",
    "            except:\n",
    "                print(\"Não existe arquivos e diretorio para exclusão\")\n",
    "\n",
    "            self.actor_model = get_actor(params)\n",
    "            self.critic_model = get_critic(params)\n",
    "            self.target_actor = get_actor(params)\n",
    "            self.target_critic = get_critic(params)\n",
    "\n",
    "            self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "            self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "\n",
    "    def policy(self, state, noise_object):\n",
    "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "        noise = noise_object()\n",
    "        # Adding noise to action\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "        # We make sure action is within bounds\n",
    "        print(sampled_actions)\n",
    "        legal_action = np.argmax(sampled_actions)\n",
    "        return [np.squeeze(legal_action)]\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "        \n",
    "\n",
    "def start_ddpg(params, env, load_model=False):\n",
    "\n",
    "    ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(params['std_dev']) * np.ones(1))\n",
    "\n",
    "    agent = DDPG(params, load_model)\n",
    "    \n",
    "    try:\n",
    "        # Iniciar Server Socket e Game Love lua\n",
    "        env.start()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Takes about 4 min to train\n",
    "        for ep in range(params['episode']):\n",
    "            prev_state, info = env.reset()\n",
    "            episodic_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "                action_policy = agent.policy(tf_prev_state, ou_noise)\n",
    "                action = action_policy[0]\n",
    "                history['action_history'].append(action)\n",
    "                    \n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                history['obs_history'].append(state)\n",
    "                history['reward_history'].append(reward)\n",
    "                \n",
    "                print(\"Episode * {} * Avg Reward is ==> {}, Action ==> {}\".format(ep, reward, action_policy[0]))\n",
    "\n",
    "                agent.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, params['tau'])\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, params['tau'])\n",
    "\n",
    "                # End this episode when `done` is True\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            history['ep_reward'].append(episodic_reward)\n",
    "            # Mean of last 40 episodes\n",
    "            avg_reward = np.mean(history['ep_reward'][-40:])\n",
    "            print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "            history['avg_reward'].append(avg_reward)\n",
    "            \n",
    "\n",
    "        # Plotting graph\n",
    "        # Episodes versus Avg. Rewards\n",
    "        plt.plot(history['avg_reward'])\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "        plt.show()\n",
    "\n",
    "    finally:\n",
    "        # Fechar Server Socket e Game Love lua\n",
    "        env.stop()\n",
    "\n",
    "        # Save the models\n",
    "        agent.actor_model.save(\"ddpg/perseguidor_actor\")\n",
    "        agent.critic_model.save(\"ddpg/perseguidor_critic\")\n",
    "        agent.target_actor.save(\"ddpg/perseguidor_target_actor\")\n",
    "        agent.target_critic.save(\"ddpg/perseguidor_target_critic\")\n",
    "\n",
    "        # Save the weights\n",
    "        agent.actor_model.save_weights(\"ddpg/perseguidor_actor.h5\")\n",
    "        agent.critic_model.save_weights(\"ddpg/perseguidor_critic.h5\")\n",
    "        agent.target_actor.save_weights(\"ddpg/perseguidor_target_actor.h5\")\n",
    "        agent.target_critic.save_weights(\"ddpg/perseguidor_target_critic.h5\")\n",
    "        \n",
    "        # Save the list \n",
    "        set_history_list('ddpg',history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53004d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden neurons (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 15:06:31.767365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 15:06:31.768494: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listining to  Address:  0.0.0.0 Port:  2223\n",
      "[0.93267281 0.93267281 0.93267281 0.93267281]\n",
      "Episode * 0 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.73846813 0.73846813 0.73846813 0.73846813]\n",
      "Episode * 0 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.879917 0.879917 0.879917 0.879917]\n",
      "Episode * 0 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.9124478 0.9124478 0.9124478 0.9124478]\n",
      "Episode * 0 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.94256921 0.94256921 0.94256921 0.94256921]\n",
      "Episode * 0 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.89212047 0.89212047 0.89212047 0.89212047]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.93942644 0.93942644 0.93942644 0.93942644]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.61254229 0.61254229 0.61254229 0.61254229]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.57929224 0.57929224 0.57929224 0.57929224]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.72276267 0.72276267 0.72276267 0.72276267]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.81049204 0.81049204 0.81049204 0.81049204]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.85589357 0.85589357 0.85589357 0.85589357]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.90318304 0.90318304 0.90318304 0.90318304]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.91735541 0.91735541 0.91735541 0.91735541]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.99899975 0.99899975 0.99899975 0.99899975]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.12389106 1.12389106 1.12389106 1.12389106]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.17783893 1.17783893 1.17783893 1.17783893]\n",
      "Episode * 0 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.15456913 1.15456913 1.15456913 1.15456913]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.03458955 1.03458955 1.03458955 1.03458955]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.02095949 1.02095949 1.02095949 1.02095949]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.03999132 1.03999132 1.03999132 1.03999132]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.02248712 1.02248712 1.02248712 1.02248712]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.22452768 1.22452768 1.22452768 1.22452768]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.25553055 1.25553055 1.25553055 1.25553055]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.36965481 1.36965481 1.36965481 1.36965481]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.20200409 1.20200409 1.20200409 1.20200409]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.313653 1.313653 1.313653 1.313653]\n",
      "Episode * 0 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.19844756 1.19844756 1.19844756 1.19844756]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.04319701 1.04319701 1.04319701 1.04319701]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.99207886 0.99207886 0.99207886 0.99207886]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.00687376 1.00687376 1.00687376 1.00687376]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.9669363 0.9669363 0.9669363 0.9669363]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.10068173 1.10068173 1.10068173 1.10068173]\n",
      "Episode * 0 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.12707418 1.12707418 1.12707418 1.12707418]\n",
      "Episode * 0 * Avg Reward is ==> -70.61131317008899, Action ==> 0\n",
      "[1.04129659 1.04129659 1.04129659 1.04129659]\n",
      "Episode * 0 * Avg Reward is ==> -71.143564884095, Action ==> 0\n",
      "[0.89823021 0.89823021 0.89823021 0.89823021]\n",
      "Episode * 0 * Avg Reward is ==> -72.031059596218, Action ==> 0\n",
      "[0.84264996 0.84264996 0.84264996 0.84264996]\n",
      "Episode * 0 * Avg Reward is ==> -72.743708078597, Action ==> 0\n",
      "[0.94840819 0.94840819 0.94840819 0.94840819]\n",
      "Episode * 0 * Avg Reward is ==> -73.46666305279099, Action ==> 0\n",
      "[0.78710459 0.78710459 0.78710459 0.78710459]\n",
      "Episode * 0 * Avg Reward is ==> -73.81243295230901, Action ==> 0\n",
      "[0.78028968 0.78028968 0.78028968 0.78028968]\n",
      "Episode * 0 * Avg Reward is ==> -74.33870094660999, Action ==> 0\n",
      "[0.81651797 0.81651797 0.81651797 0.81651797]\n",
      "Episode * 0 * Avg Reward is ==> -75.046502408229, Action ==> 0\n",
      "[0.82451536 0.82451536 0.82451536 0.82451536]\n",
      "Episode * 0 * Avg Reward is ==> -75.578121363047, Action ==> 0\n",
      "[0.72957308 0.72957308 0.72957308 0.72957308]\n",
      "Episode * 0 * Avg Reward is ==> -75.932076457408, Action ==> 0\n",
      "[0.76261764 0.76261764 0.76261764 0.76261764]\n",
      "Episode * 0 * Avg Reward is ==> -77.178844080886, Action ==> 0\n",
      "[0.72302318 0.72302318 0.72302318 0.72302318]\n",
      "Episode * 0 * Avg Reward is ==> -77.355649966288, Action ==> 0\n",
      "[0.69088956 0.69088956 0.69088956 0.69088956]\n",
      "Episode * 0 * Avg Reward is ==> -78.07146347723399, Action ==> 0\n",
      "[0.55806298 0.55806298 0.55806298 0.55806298]\n",
      "Episode * 0 * Avg Reward is ==> -78.419720843177, Action ==> 0\n",
      "[0.42302882 0.42302882 0.42302882 0.42302882]\n",
      "Episode * 0 * Avg Reward is ==> -79.12968399734, Action ==> 0\n",
      "[0.32455398 0.32455398 0.32455398 0.32455398]\n",
      "Episode * 0 * Avg Reward is ==> -79.489998730033, Action ==> 0\n",
      "[0.45523487 0.45523487 0.45523487 0.45523487]\n",
      "Episode * 0 * Avg Reward is ==> -80.190777213869, Action ==> 0\n",
      "[0.42145084 0.42145084 0.42145084 0.42145084]\n",
      "Episode * 0 * Avg Reward is ==> -80.722317743391, Action ==> 0\n",
      "[0.32887344 0.32887344 0.32887344 0.32887344]\n",
      "Episode * 0 * Avg Reward is ==> -81.078318318361, Action ==> 0\n",
      "[0.44874736 0.44874736 0.44874736 0.44874736]\n",
      "Episode * 0 * Avg Reward is ==> -81.788129479366, Action ==> 0\n",
      "[0.54901002 0.54901002 0.54901002 0.54901002]\n",
      "Episode * 0 * Avg Reward is ==> -82.144615483363, Action ==> 0\n",
      "[0.59197882 0.59197882 0.59197882 0.59197882]\n",
      "Episode * 0 * Avg Reward is ==> -82.500058333736, Action ==> 0\n",
      "[0.73781453 0.73781453 0.73781453 0.73781453]\n",
      "Episode * 0 * Avg Reward is ==> -82.85251852641, Action ==> 0\n",
      "[0.58663405 0.58663405 0.58663405 0.58663405]\n",
      "Episode * 0 * Avg Reward is ==> -83.383861632099, Action ==> 0\n",
      "[0.62339523 0.62339523 0.62339523 0.62339523]\n",
      "Episode * 0 * Avg Reward is ==> -83.742081478532, Action ==> 0\n",
      "[0.73086333 0.73086333 0.73086333 0.73086333]\n",
      "Episode * 0 * Avg Reward is ==> -84.62744161554299, Action ==> 0\n",
      "[0.94043122 0.94043122 0.94043122 0.94043122]\n",
      "Episode * 0 * Avg Reward is ==> -84.80400862014899, Action ==> 0\n",
      "[0.97346356 0.97346356 0.97346356 0.97346356]\n",
      "Episode * 0 * Avg Reward is ==> -85.336976541353, Action ==> 0\n",
      "[0.94955837 0.94955837 0.94955837 0.94955837]\n",
      "Episode * 0 * Avg Reward is ==> -86.045386178888, Action ==> 0\n",
      "[0.87111728 0.87111728 0.87111728 0.87111728]\n",
      "Episode * 0 * Avg Reward is ==> -86.930798064477, Action ==> 0\n",
      "[0.9399427 0.9399427 0.9399427 0.9399427]\n",
      "Episode * 0 * Avg Reward is ==> -87.64088443447599, Action ==> 0\n",
      "[0.73612866 0.73612866 0.73612866 0.73612866]\n",
      "Episode * 0 * Avg Reward is ==> -88.604079147799, Action ==> 0\n",
      "[0.6891055 0.6891055 0.6891055 0.6891055]\n",
      "Episode * 0 * Avg Reward is ==> -90.5551860879953, Action ==> 0\n",
      "[0.71068459 0.71068459 0.71068459 0.71068459]\n",
      "Episode * 0 * Avg Reward is ==> -91.2639180684764, Action ==> 0\n",
      "[0.7117694 0.7117694 0.7117694 0.7117694]\n",
      "Episode * 0 * Avg Reward is ==> -91.80220629686791, Action ==> 0\n",
      "[0.73859585 0.73859585 0.73859585 0.73859585]\n",
      "Episode * 0 * Avg Reward is ==> -92.9162465775172, Action ==> 0\n",
      "[0.87629918 0.87629918 0.87629918 0.87629918]\n",
      "Episode * 0 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 0 * Avg Reward is ==> -2831.27931384702\n",
      "[0.78668047 0.78668047 0.78668047 0.78668047]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.78756141 0.78756141 0.78756141 0.78756141]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.93714951 0.93714951 0.93714951 0.93714951]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.89753273 0.89753273 0.89753273 0.89753273]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.95736106 0.95736106 0.95736106 0.95736106]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.99944299 0.99944299 0.99944299 0.99944299]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.14416673 1.14416673 1.14416673 1.14416673]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.20021522 1.20021522 1.20021522 1.20021522]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.15928284 1.15928284 1.15928284 1.15928284]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.97143487 0.97143487 0.97143487 0.97143487]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.98911005 0.98911005 0.98911005 0.98911005]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02270918 1.02270918 1.02270918 1.02270918]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.7824376 0.7824376 0.7824376 0.7824376]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.84739704 0.84739704 0.84739704 0.84739704]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.97361648 0.97361648 0.97361648 0.97361648]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.94119834 0.94119834 0.94119834 0.94119834]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.98838331 0.98838331 0.98838331 0.98838331]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.00676662 1.00676662 1.00676662 1.00676662]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.93932188 0.93932188 0.93932188 0.93932188]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.79486734 0.79486734 0.79486734 0.79486734]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.85052335 0.85052335 0.85052335 0.85052335]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.0800587 1.0800587 1.0800587 1.0800587]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.83661925 0.83661925 0.83661925 0.83661925]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.68793076 0.68793076 0.68793076 0.68793076]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.81494898 0.81494898 0.81494898 0.81494898]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.94903573 0.94903573 0.94903573 0.94903573]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[1.10315508 1.10315508 1.10315508 1.10315508]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[1.12509307 1.12509307 1.12509307 1.12509307]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[1.07533828 1.07533828 1.07533828 1.07533828]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[1.11843279 1.11843279 1.11843279 1.11843279]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[1.02422281 1.02422281 1.02422281 1.02422281]\n",
      "Episode * 1 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.97797387 0.97797387 0.97797387 0.97797387]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.07815687 1.07815687 1.07815687 1.07815687]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.2881551 1.2881551 1.2881551 1.2881551]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.29268932 1.29268932 1.29268932 1.29268932]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.41097223 1.41097223 1.41097223 1.41097223]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.4173927 1.4173927 1.4173927 1.4173927]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.42489572 1.42489572 1.42489572 1.42489572]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.32312164 1.32312164 1.32312164 1.32312164]\n",
      "Episode * 1 * Avg Reward is ==> 10, Action ==> 0\n",
      "[1.34800794 1.34800794 1.34800794 1.34800794]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[1.26699929 1.26699929 1.26699929 1.26699929]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[1.0945742 1.0945742 1.0945742 1.0945742]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.91182066 0.91182066 0.91182066 0.91182066]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[1.00646048 1.00646048 1.00646048 1.00646048]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.91248479 0.91248479 0.91248479 0.91248479]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.92220226 0.92220226 0.92220226 0.92220226]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.89257208 0.89257208 0.89257208 0.89257208]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.75455595 0.75455595 0.75455595 0.75455595]\n",
      "Episode * 1 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.89516278 0.89516278 0.89516278 0.89516278]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.86616182 0.86616182 0.86616182 0.86616182]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.85978599 0.85978599 0.85978599 0.85978599]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.82591561 0.82591561 0.82591561 0.82591561]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.79636108 0.79636108 0.79636108 0.79636108]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.90569652 0.90569652 0.90569652 0.90569652]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[1.07605261 1.07605261 1.07605261 1.07605261]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[1.22593185 1.22593185 1.22593185 1.22593185]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.9887322 0.9887322 0.9887322 0.9887322]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.96091149 0.96091149 0.96091149 0.96091149]\n",
      "Episode * 1 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.90298043 0.90298043 0.90298043 0.90298043]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.9511433 0.9511433 0.9511433 0.9511433]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.81503431 0.81503431 0.81503431 0.81503431]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.95261409 0.95261409 0.95261409 0.95261409]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.03165283 1.03165283 1.03165283 1.03165283]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.94774825 0.94774825 0.94774825 0.94774825]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.82669097 0.82669097 0.82669097 0.82669097]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.90408458 0.90408458 0.90408458 0.90408458]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.85236914 0.85236914 0.85236914 0.85236914]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.71960369 0.71960369 0.71960369 0.71960369]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.85808127 0.85808127 0.85808127 0.85808127]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.95873838 0.95873838 0.95873838 0.95873838]\n",
      "Episode * 1 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.88736839 0.88736839 0.88736839 0.88736839]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.09568594 1.09568594 1.09568594 1.09568594]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.9830748 0.9830748 0.9830748 0.9830748]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.12445156 1.12445156 1.12445156 1.12445156]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.3413888 1.3413888 1.3413888 1.3413888]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.32441434 1.32441434 1.32441434 1.32441434]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.12191267 1.12191267 1.12191267 1.12191267]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.1344134 1.1344134 1.1344134 1.1344134]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.20367732 1.20367732 1.20367732 1.20367732]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.17496022 1.17496022 1.17496022 1.17496022]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.96335009 0.96335009 0.96335009 0.96335009]\n",
      "Episode * 1 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.88983167 0.88983167 0.88983167 0.88983167]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.012901 1.012901 1.012901 1.012901]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.97952626 0.97952626 0.97952626 0.97952626]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.14631844 1.14631844 1.14631844 1.14631844]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.10956077 1.10956077 1.10956077 1.10956077]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.00170468 1.00170468 1.00170468 1.00170468]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.00919544 1.00919544 1.00919544 1.00919544]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.09539967 1.09539967 1.09539967 1.09539967]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.35528318 1.35528318 1.35528318 1.35528318]\n",
      "Episode * 1 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.23322251 1.23322251 1.23322251 1.23322251]\n",
      "Episode * 1 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.29704146 1.29704146 1.29704146 1.29704146]\n",
      "Episode * 1 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.34351649 1.34351649 1.34351649 1.34351649]\n",
      "Episode * 1 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.3729276 1.3729276 1.3729276 1.3729276]\n",
      "Episode * 1 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.3133929 1.3133929 1.3133929 1.3133929]\n",
      "Episode * 1 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.36011075 1.36011075 1.36011075 1.36011075]\n",
      "Episode * 1 * Avg Reward is ==> -70.560318010117, Action ==> 0\n",
      "[1.30418648 1.30418648 1.30418648 1.30418648]\n",
      "Episode * 1 * Avg Reward is ==> -71.80352307694, Action ==> 0\n",
      "[1.12684115 1.12684115 1.12684115 1.12684115]\n",
      "Episode * 1 * Avg Reward is ==> -72.347329865196, Action ==> 0\n",
      "[1.06723529 1.06723529 1.06723529 1.06723529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 1 * Avg Reward is ==> -72.690053929796, Action ==> 0\n",
      "[1.11577338 1.11577338 1.11577338 1.11577338]\n",
      "Episode * 1 * Avg Reward is ==> -73.5799932112, Action ==> 0\n",
      "[1.19130099 1.19130099 1.19130099 1.19130099]\n",
      "Episode * 1 * Avg Reward is ==> -74.141228427922, Action ==> 0\n",
      "[1.15698649 1.15698649 1.15698649 1.15698649]\n",
      "Episode * 1 * Avg Reward is ==> -74.817368061167, Action ==> 0\n",
      "[1.26327054 1.26327054 1.26327054 1.26327054]\n",
      "Episode * 1 * Avg Reward is ==> -75.372029418297, Action ==> 0\n",
      "[1.26294966 1.26294966 1.26294966 1.26294966]\n",
      "Episode * 1 * Avg Reward is ==> -76.23717231529, Action ==> 0\n",
      "[1.208036 1.208036 1.208036 1.208036]\n",
      "Episode * 1 * Avg Reward is ==> -76.95505089787599, Action ==> 0\n",
      "[0.95613641 0.95613641 0.95613641 0.95613641]\n",
      "Episode * 1 * Avg Reward is ==> -77.303446408466, Action ==> 0\n",
      "[0.9045536 0.9045536 0.9045536 0.9045536]\n",
      "Episode * 1 * Avg Reward is ==> -77.672323520452, Action ==> 0\n",
      "[1.00424558 1.00424558 1.00424558 1.00424558]\n",
      "Episode * 1 * Avg Reward is ==> -79.252446599783, Action ==> 0\n",
      "[0.86283742 0.86283742 0.86283742 0.86283742]\n",
      "Episode * 1 * Avg Reward is ==> -80.072844946692, Action ==> 0\n",
      "[0.92406414 0.92406414 0.92406414 0.92406414]\n",
      "Episode * 1 * Avg Reward is ==> -80.494000447822, Action ==> 0\n",
      "[0.9875358 0.9875358 0.9875358 0.9875358]\n",
      "Episode * 1 * Avg Reward is ==> -80.866664110382, Action ==> 0\n",
      "[1.0374167 1.0374167 1.0374167 1.0374167]\n",
      "Episode * 1 * Avg Reward is ==> -81.91349076628, Action ==> 0\n",
      "[0.87717336 0.87717336 0.87717336 0.87717336]\n",
      "Episode * 1 * Avg Reward is ==> -83.355375270995, Action ==> 0\n",
      "[0.81401549 0.81401549 0.81401549 0.81401549]\n",
      "Episode * 1 * Avg Reward is ==> -84.396682325794, Action ==> 0\n",
      "[0.79189245 0.79189245 0.79189245 0.79189245]\n",
      "Episode * 1 * Avg Reward is ==> -85.824535557245, Action ==> 0\n",
      "[0.70335226 0.70335226 0.70335226 0.70335226]\n",
      "Episode * 1 * Avg Reward is ==> -86.70371502398, Action ==> 0\n",
      "[0.7825189 0.7825189 0.7825189 0.7825189]\n",
      "Episode * 1 * Avg Reward is ==> -87.963337597065, Action ==> 0\n",
      "[0.89339699 0.89339699 0.89339699 0.89339699]\n",
      "Episode * 1 * Avg Reward is ==> -88.558634084251, Action ==> 0\n",
      "[0.91485671 0.91485671 0.91485671 0.91485671]\n",
      "Episode * 1 * Avg Reward is ==> -89.898418571447, Action ==> 0\n",
      "[0.95354005 0.95354005 0.95354005 0.95354005]\n",
      "Episode * 1 * Avg Reward is ==> -90.6041253488885, Action ==> 0\n",
      "[0.94752022 0.94752022 0.94752022 0.94752022]\n",
      "Episode * 1 * Avg Reward is ==> -91.3136491201326, Action ==> 0\n",
      "[1.00043444 1.00043444 1.00043444 1.00043444]\n",
      "Episode * 1 * Avg Reward is ==> -92.3775997442617, Action ==> 0\n",
      "[1.14619288 1.14619288 1.14619288 1.14619288]\n",
      "Episode * 1 * Avg Reward is ==> -93.08732050709439, Action ==> 0\n",
      "[1.29381265 1.29381265 1.29381265 1.29381265]\n",
      "Episode * 1 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 1 * Avg Reward is ==> -2212.720995505926\n",
      "[1.44809676 1.44809676 1.44809676 1.44809676]\n",
      "Episode * 2 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.35131788 1.35131788 1.35131788 1.35131788]\n",
      "Episode * 2 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.40614138 1.40614138 1.40614138 1.40614138]\n",
      "Episode * 2 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.5248778 1.5248778 1.5248778 1.5248778]\n",
      "Episode * 2 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.37818204 1.37818204 1.37818204 1.37818204]\n",
      "Episode * 2 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.416233 1.416233 1.416233 1.416233]\n",
      "Episode * 2 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.40651371 1.40651371 1.40651371 1.40651371]\n",
      "Episode * 2 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.34038854 1.34038854 1.34038854 1.34038854]\n",
      "Episode * 2 * Avg Reward is ==> -71.335001647787, Action ==> 0\n",
      "[1.45674216 1.45674216 1.45674216 1.45674216]\n",
      "Episode * 2 * Avg Reward is ==> -72.843188178707, Action ==> 0\n",
      "[1.30926884 1.30926884 1.30926884 1.30926884]\n",
      "Episode * 2 * Avg Reward is ==> -74.293190104969, Action ==> 0\n",
      "[1.23685932 1.23685932 1.23685932 1.23685932]\n",
      "Episode * 2 * Avg Reward is ==> -75.67615190711801, Action ==> 0\n",
      "[1.17046195 1.17046195 1.17046195 1.17046195]\n",
      "Episode * 2 * Avg Reward is ==> -76.966711464047, Action ==> 0\n",
      "[1.33585795 1.33585795 1.33585795 1.33585795]\n",
      "Episode * 2 * Avg Reward is ==> -78.172385495022, Action ==> 0\n",
      "[1.30447337 1.30447337 1.30447337 1.30447337]\n",
      "Episode * 2 * Avg Reward is ==> -79.512280817674, Action ==> 0\n",
      "[1.01339986 1.01339986 1.01339986 1.01339986]\n",
      "Episode * 2 * Avg Reward is ==> -80.20332472004, Action ==> 0\n",
      "[0.84096955 0.84096955 0.84096955 0.84096955]\n",
      "Episode * 2 * Avg Reward is ==> -80.817687996017, Action ==> 0\n",
      "[0.87932441 0.87932441 0.87932441 0.87932441]\n",
      "Episode * 2 * Avg Reward is ==> -81.33664448837, Action ==> 0\n",
      "[0.69395732 0.69395732 0.69395732 0.69395732]\n",
      "Episode * 2 * Avg Reward is ==> -81.753474207538, Action ==> 0\n",
      "[0.54475339 0.54475339 0.54475339 0.54475339]\n",
      "Episode * 2 * Avg Reward is ==> -81.97133975426101, Action ==> 0\n",
      "[0.48467722 0.48467722 0.48467722 0.48467722]\n",
      "Episode * 2 * Avg Reward is ==> -82.137187989215, Action ==> 0\n",
      "[0.44404526 0.44404526 0.44404526 0.44404526]\n",
      "Episode * 2 * Avg Reward is ==> -82.3130320085, Action ==> 0\n",
      "[0.47849268 0.47849268 0.47849268 0.47849268]\n",
      "Episode * 2 * Avg Reward is ==> -82.320151032579, Action ==> 0\n",
      "[0.44116008 0.44116008 0.44116008 0.44116008]\n",
      "Episode * 2 * Avg Reward is ==> -82.279407974071, Action ==> 0\n",
      "[0.40954136 0.40954136 0.40954136 0.40954136]\n",
      "Episode * 2 * Avg Reward is ==> -81.90337211934, Action ==> 0\n",
      "[0.39456195 0.39456195 0.39456195 0.39456195]\n",
      "Episode * 2 * Avg Reward is ==> -81.33843884557099, Action ==> 0\n",
      "[0.35078745 0.35078745 0.35078745 0.35078745]\n",
      "Episode * 2 * Avg Reward is ==> -80.84001097256, Action ==> 0\n",
      "[0.26785897 0.26785897 0.26785897 0.26785897]\n",
      "Episode * 2 * Avg Reward is ==> -80.463065761398, Action ==> 0\n",
      "[0.14210139 0.14210139 0.14210139 0.14210139]\n",
      "Episode * 2 * Avg Reward is ==> -80.207136044482, Action ==> 0\n",
      "[0.34484274 0.34484274 0.34484274 0.34484274]\n",
      "Episode * 2 * Avg Reward is ==> -80.271195330718, Action ==> 0\n",
      "[0.32802605 0.32802605 0.32802605 0.32802605]\n",
      "Episode * 2 * Avg Reward is ==> -80.840032782417, Action ==> 0\n",
      "[0.217345 0.217345 0.217345 0.217345]\n",
      "Episode * 2 * Avg Reward is ==> -81.396108574811, Action ==> 0\n",
      "[0.0640693 0.0640693 0.0640693 0.0640693]\n",
      "Episode * 2 * Avg Reward is ==> -82.115342643917, Action ==> 0\n",
      "[-0.03274597 -0.03274597 -0.03274597 -0.03274597]\n",
      "Episode * 2 * Avg Reward is ==> -82.503547813722, Action ==> 0\n",
      "[0.0466004 0.0466004 0.0466004 0.0466004]\n",
      "Episode * 2 * Avg Reward is ==> -83.361839789642, Action ==> 0\n",
      "[0.3059978 0.3059978 0.3059978 0.3059978]\n",
      "Episode * 2 * Avg Reward is ==> -84.404606302446, Action ==> 0\n",
      "[0.23742679 0.23742679 0.23742679 0.23742679]\n",
      "Episode * 2 * Avg Reward is ==> -85.660918551503, Action ==> 0\n",
      "[0.28759044 0.28759044 0.28759044 0.28759044]\n",
      "Episode * 2 * Avg Reward is ==> -86.457558910015, Action ==> 0\n",
      "[0.24204019 0.24204019 0.24204019 0.24204019]\n",
      "Episode * 2 * Avg Reward is ==> -87.25828115248599, Action ==> 0\n",
      "[0.30006954 0.30006954 0.30006954 0.30006954]\n",
      "Episode * 2 * Avg Reward is ==> -88.322990903587, Action ==> 0\n",
      "[0.35181996 0.35181996 0.35181996 0.35181996]\n",
      "Episode * 2 * Avg Reward is ==> -89.387845842918, Action ==> 0\n",
      "[0.3771286 0.3771286 0.3771286 0.3771286]\n",
      "Episode * 2 * Avg Reward is ==> -90.1002087494134, Action ==> 0\n",
      "[0.27947692 0.27947692 0.27947692 0.27947692]\n",
      "Episode * 2 * Avg Reward is ==> -90.9948484207431, Action ==> 0\n",
      "[0.26456673 0.26456673 0.26456673 0.26456673]\n",
      "Episode * 2 * Avg Reward is ==> -91.87467089928319, Action ==> 0\n",
      "[0.3350543 0.3350543 0.3350543 0.3350543]\n",
      "Episode * 2 * Avg Reward is ==> -92.5824009095109, Action ==> 0\n",
      "[0.23908454 0.23908454 0.23908454 0.23908454]\n",
      "Episode * 2 * Avg Reward is ==> -93.4748607992573, Action ==> 0\n",
      "[0.11930979 0.11930979 0.11930979 0.11930979]\n",
      "Episode * 2 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 2 * Avg Reward is ==> -2543.7108109725027\n",
      "[0.21525415 0.21525415 0.21525415 0.21525415]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.26247101 0.26247101 0.26247101 0.26247101]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.24729466 0.24729466 0.24729466 0.24729466]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.16360154 0.16360154 0.16360154 0.16360154]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.10009098 0.10009098 0.10009098 0.10009098]\n",
      "Episode * 3 * Avg Reward is ==> 8, Action ==> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27239579 0.27239579 0.27239579 0.27239579]\n",
      "Episode * 3 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.19048856 0.19048856 0.19048856 0.19048856]\n",
      "Episode * 3 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.24231202 0.24231202 0.24231202 0.24231202]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.33438634 0.33438634 0.33438634 0.33438634]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.32975932 0.32975932 0.32975932 0.32975932]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.4308265 0.4308265 0.4308265 0.4308265]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.31671114 0.31671114 0.31671114 0.31671114]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.28907598 0.28907598 0.28907598 0.28907598]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.19772935 0.19772935 0.19772935 0.19772935]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.24718072 0.24718072 0.24718072 0.24718072]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.17622332 0.17622332 0.17622332 0.17622332]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.15820056 0.15820056 0.15820056 0.15820056]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.32046419 0.32046419 0.32046419 0.32046419]\n",
      "Episode * 3 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.2225005 0.2225005 0.2225005 0.2225005]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.1601931 0.1601931 0.1601931 0.1601931]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.03208393 0.03208393 0.03208393 0.03208393]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.16431521 0.16431521 0.16431521 0.16431521]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.21647581 0.21647581 0.21647581 0.21647581]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.15905118 0.15905118 0.15905118 0.15905118]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.0519447 -0.0519447 -0.0519447 -0.0519447]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.10462742 0.10462742 0.10462742 0.10462742]\n",
      "Episode * 3 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.10477919 0.10477919 0.10477919 0.10477919]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.24096159 0.24096159 0.24096159 0.24096159]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.22930349 0.22930349 0.22930349 0.22930349]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.21517779 0.21517779 0.21517779 0.21517779]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.07780578 0.07780578 0.07780578 0.07780578]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.12317831 0.12317831 0.12317831 0.12317831]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.23304667 0.23304667 0.23304667 0.23304667]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.19175293 0.19175293 0.19175293 0.19175293]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.13521329 0.13521329 0.13521329 0.13521329]\n",
      "Episode * 3 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.01393644 0.01393644 0.01393644 0.01393644]\n",
      "Episode * 3 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.05272907 -0.05272907 -0.05272907 -0.05272907]\n",
      "Episode * 3 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.01050958 0.01050958 0.01050958 0.01050958]\n",
      "Episode * 3 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.08625183 0.08625183 0.08625183 0.08625183]\n",
      "Episode * 3 * Avg Reward is ==> -70.295414169308, Action ==> 0\n",
      "[0.14220272 0.14220272 0.14220272 0.14220272]\n",
      "Episode * 3 * Avg Reward is ==> -71.18319426453999, Action ==> 0\n",
      "[0.12521361 0.12521361 0.12521361 0.12521361]\n",
      "Episode * 3 * Avg Reward is ==> -72.24704721638899, Action ==> 0\n",
      "[0.17569923 0.17569923 0.17569923 0.17569923]\n",
      "Episode * 3 * Avg Reward is ==> -73.489973098218, Action ==> 0\n",
      "[0.21161211 0.21161211 0.21161211 0.21161211]\n",
      "Episode * 3 * Avg Reward is ==> -75.098751901475, Action ==> 0\n",
      "[0.20778863 0.20778863 0.20778863 0.20778863]\n",
      "Episode * 3 * Avg Reward is ==> -76.18125983481501, Action ==> 0\n",
      "[0.17912268 0.17912268 0.17912268 0.17912268]\n",
      "Episode * 3 * Avg Reward is ==> -77.388664021035, Action ==> 0\n",
      "[0.15888881 0.15888881 0.15888881 0.15888881]\n",
      "Episode * 3 * Avg Reward is ==> -78.808165852093, Action ==> 0\n",
      "[0.28302562 0.28302562 0.28302562 0.28302562]\n",
      "Episode * 3 * Avg Reward is ==> -80.04987628110001, Action ==> 0\n",
      "[0.17040452 0.17040452 0.17040452 0.17040452]\n",
      "Episode * 3 * Avg Reward is ==> -81.127053115405, Action ==> 0\n",
      "[0.12174261 0.12174261 0.12174261 0.12174261]\n",
      "Episode * 3 * Avg Reward is ==> -82.533425359696, Action ==> 0\n",
      "[0.21916496 0.21916496 0.21916496 0.21916496]\n",
      "Episode * 3 * Avg Reward is ==> -83.953202868915, Action ==> 0\n",
      "[0.16248999 0.16248999 0.16248999 0.16248999]\n",
      "Episode * 3 * Avg Reward is ==> -85.20220750598399, Action ==> 0\n",
      "[0.2717844 0.2717844 0.2717844 0.2717844]\n",
      "Episode * 3 * Avg Reward is ==> -87.14479086090499, Action ==> 0\n",
      "[0.30551112 0.30551112 0.30551112 0.30551112]\n",
      "Episode * 3 * Avg Reward is ==> -88.387555028429, Action ==> 0\n",
      "[0.25953552 0.25953552 0.25953552 0.25953552]\n",
      "Episode * 3 * Avg Reward is ==> -89.277615388464, Action ==> 0\n",
      "[0.12215087 0.12215087 0.12215087 0.12215087]\n",
      "Episode * 3 * Avg Reward is ==> -90.8739069098186, Action ==> 0\n",
      "[0.08191836 0.08191836 0.08191836 0.08191836]\n",
      "Episode * 3 * Avg Reward is ==> -92.13520187686231, Action ==> 0\n",
      "[-0.08642311 -0.08642311 -0.08642311 -0.08642311]\n",
      "Episode * 3 * Avg Reward is ==> -93.3545373590872, Action ==> 0\n",
      "[0.15348801 0.15348801 0.15348801 0.15348801]\n",
      "Episode * 3 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 3 * Avg Reward is ==> -2261.466068957512\n",
      "[0.14377904 0.14377904 0.14377904 0.14377904]\n",
      "Episode * 4 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.13497298 0.13497298 0.13497298 0.13497298]\n",
      "Episode * 4 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.09305368 0.09305368 0.09305368 0.09305368]\n",
      "Episode * 4 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.03928413 0.03928413 0.03928413 0.03928413]\n",
      "Episode * 4 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.01560736 0.01560736 0.01560736 0.01560736]\n",
      "Episode * 4 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.061556 -0.061556 -0.061556 -0.061556]\n",
      "Episode * 4 * Avg Reward is ==> -70.062271678793, Action ==> 0\n",
      "[-0.18192894 -0.18192894 -0.18192894 -0.18192894]\n",
      "Episode * 4 * Avg Reward is ==> -72.528914399192, Action ==> 0\n",
      "[-0.20262096 -0.20262096 -0.20262096 -0.20262096]\n",
      "Episode * 4 * Avg Reward is ==> -75.191253557938, Action ==> 0\n",
      "[-0.31550998 -0.31550998 -0.31550998 -0.31550998]\n",
      "Episode * 4 * Avg Reward is ==> -77.706781298465, Action ==> 0\n",
      "[-0.20381635 -0.20381635 -0.20381635 -0.20381635]\n",
      "Episode * 4 * Avg Reward is ==> -80.534936588174, Action ==> 0\n",
      "[-0.26820678 -0.26820678 -0.26820678 -0.26820678]\n",
      "Episode * 4 * Avg Reward is ==> -82.50182101493, Action ==> 0\n",
      "[-0.26913757 -0.26913757 -0.26913757 -0.26913757]\n",
      "Episode * 4 * Avg Reward is ==> -83.51381175857999, Action ==> 0\n",
      "[-0.41876852 -0.41876852 -0.41876852 -0.41876852]\n",
      "Episode * 4 * Avg Reward is ==> -84.113343293922, Action ==> 0\n",
      "[-0.67279308 -0.67279308 -0.67279308 -0.67279308]\n",
      "Episode * 4 * Avg Reward is ==> -84.00042242743, Action ==> 0\n",
      "[-0.85527906 -0.85527906 -0.85527906 -0.85527906]\n",
      "Episode * 4 * Avg Reward is ==> -82.920443149321, Action ==> 0\n",
      "[-0.62221348 -0.62221348 -0.62221348 -0.62221348]\n",
      "Episode * 4 * Avg Reward is ==> -82.41049462328401, Action ==> 0\n",
      "[-0.780913 -0.780913 -0.780913 -0.780913]\n",
      "Episode * 4 * Avg Reward is ==> -81.372549856384, Action ==> 0\n",
      "[-0.78667007 -0.78667007 -0.78667007 -0.78667007]\n",
      "Episode * 4 * Avg Reward is ==> -80.834037808671, Action ==> 0\n",
      "[-0.73756296 -0.73756296 -0.73756296 -0.73756296]\n",
      "Episode * 4 * Avg Reward is ==> -81.748871991973, Action ==> 0\n",
      "[-0.66942788 -0.66942788 -0.66942788 -0.66942788]\n",
      "Episode * 4 * Avg Reward is ==> -83.298399576473, Action ==> 0\n",
      "[-0.57677944 -0.57677944 -0.57677944 -0.57677944]\n",
      "Episode * 4 * Avg Reward is ==> -84.36581316847, Action ==> 0\n",
      "[-0.66787646 -0.66787646 -0.66787646 -0.66787646]\n",
      "Episode * 4 * Avg Reward is ==> -85.235647620001, Action ==> 0\n",
      "[-0.4446345 -0.4446345 -0.4446345 -0.4446345]\n",
      "Episode * 4 * Avg Reward is ==> -85.945349778231, Action ==> 0\n",
      "[-0.52248598 -0.52248598 -0.52248598 -0.52248598]\n",
      "Episode * 4 * Avg Reward is ==> -87.033512125999, Action ==> 0\n",
      "[-0.66587685 -0.66587685 -0.66587685 -0.66587685]\n",
      "Episode * 4 * Avg Reward is ==> -88.627612744564, Action ==> 0\n",
      "[-0.62525401 -0.62525401 -0.62525401 -0.62525401]\n",
      "Episode * 4 * Avg Reward is ==> -90.5574945027151, Action ==> 0\n",
      "[-0.55674071 -0.55674071 -0.55674071 -0.55674071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 4 * Avg Reward is ==> -91.27321830573611, Action ==> 0\n",
      "[-0.57369217 -0.57369217 -0.57369217 -0.57369217]\n",
      "Episode * 4 * Avg Reward is ==> -92.1570202123526, Action ==> 0\n",
      "[-0.52162561 -0.52162561 -0.52162561 -0.52162561]\n",
      "Episode * 4 * Avg Reward is ==> -93.04449585365249, Action ==> 0\n",
      "[-0.61692156 -0.61692156 -0.61692156 -0.61692156]\n",
      "Episode * 4 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 4 * Avg Reward is ==> -2224.1685586330595\n",
      "[-0.61875792 -0.61875792 -0.61875792 -0.61875792]\n",
      "Episode * 5 * Avg Reward is ==> 11, Action ==> 0\n",
      "[-0.59741997 -0.59741997 -0.59741997 -0.59741997]\n",
      "Episode * 5 * Avg Reward is ==> 11, Action ==> 0\n",
      "[-0.80616796 -0.80616796 -0.80616796 -0.80616796]\n",
      "Episode * 5 * Avg Reward is ==> 11, Action ==> 0\n",
      "[-0.81707939 -0.81707939 -0.81707939 -0.81707939]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.77971604 -0.77971604 -0.77971604 -0.77971604]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.93633699 -0.93633699 -0.93633699 -0.93633699]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.85639846 -0.85639846 -0.85639846 -0.85639846]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.89473396 -0.89473396 -0.89473396 -0.89473396]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.9293697 -0.9293697 -0.9293697 -0.9293697]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-1.07422947 -1.07422947 -1.07422947 -1.07422947]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-1.05664051 -1.05664051 -1.05664051 -1.05664051]\n",
      "Episode * 5 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-1.17336633 -1.17336633 -1.17336633 -1.17336633]\n",
      "Episode * 5 * Avg Reward is ==> 9, Action ==> 0\n",
      "[-1.03742657 -1.03742657 -1.03742657 -1.03742657]\n",
      "Episode * 5 * Avg Reward is ==> 9, Action ==> 0\n",
      "[-0.87814979 -0.87814979 -0.87814979 -0.87814979]\n",
      "Episode * 5 * Avg Reward is ==> 9, Action ==> 0\n",
      "[-0.72853764 -0.72853764 -0.72853764 -0.72853764]\n",
      "Episode * 5 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.70395647 -0.70395647 -0.70395647 -0.70395647]\n",
      "Episode * 5 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.66736044 -0.66736044 -0.66736044 -0.66736044]\n",
      "Episode * 5 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.67760434 -0.67760434 -0.67760434 -0.67760434]\n",
      "Episode * 5 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.68723463 -0.68723463 -0.68723463 -0.68723463]\n",
      "Episode * 5 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.8293341 -0.8293341 -0.8293341 -0.8293341]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.04197067 -1.04197067 -1.04197067 -1.04197067]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.07911946 -1.07911946 -1.07911946 -1.07911946]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.06574201 -1.06574201 -1.06574201 -1.06574201]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.1325497 -1.1325497 -1.1325497 -1.1325497]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.22829388 -1.22829388 -1.22829388 -1.22829388]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.21560503 -1.21560503 -1.21560503 -1.21560503]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.20586317 -1.20586317 -1.20586317 -1.20586317]\n",
      "Episode * 5 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-1.25242885 -1.25242885 -1.25242885 -1.25242885]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.28437544 -1.28437544 -1.28437544 -1.28437544]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.42229086 -1.42229086 -1.42229086 -1.42229086]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.4775155 -1.4775155 -1.4775155 -1.4775155]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.51481517 -1.51481517 -1.51481517 -1.51481517]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.54951553 -1.54951553 -1.54951553 -1.54951553]\n",
      "Episode * 5 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-1.30801429 -1.30801429 -1.30801429 -1.30801429]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-1.19180022 -1.19180022 -1.19180022 -1.19180022]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-1.17255423 -1.17255423 -1.17255423 -1.17255423]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-1.04918316 -1.04918316 -1.04918316 -1.04918316]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.9298184 -0.9298184 -0.9298184 -0.9298184]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.94117464 -0.94117464 -0.94117464 -0.94117464]\n",
      "Episode * 5 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.9796665 -0.9796665 -0.9796665 -0.9796665]\n",
      "Episode * 5 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.897337 -0.897337 -0.897337 -0.897337]\n",
      "Episode * 5 * Avg Reward is ==> -70.252626295485, Action ==> 0\n",
      "[-0.85035994 -0.85035994 -0.85035994 -0.85035994]\n",
      "Episode * 5 * Avg Reward is ==> -71.9558917746, Action ==> 0\n",
      "[-0.86223228 -0.86223228 -0.86223228 -0.86223228]\n",
      "Episode * 5 * Avg Reward is ==> -74.27538316497801, Action ==> 0\n",
      "[-0.82891073 -0.82891073 -0.82891073 -0.82891073]\n",
      "Episode * 5 * Avg Reward is ==> -76.401442456128, Action ==> 0\n",
      "[-0.99529646 -0.99529646 -0.99529646 -0.99529646]\n",
      "Episode * 5 * Avg Reward is ==> -79.05278824141399, Action ==> 0\n",
      "[-0.82218642 -0.82218642 -0.82218642 -0.82218642]\n",
      "Episode * 5 * Avg Reward is ==> -80.500212056653, Action ==> 0\n",
      "[-0.97482206 -0.97482206 -0.97482206 -0.97482206]\n",
      "Episode * 5 * Avg Reward is ==> -82.29217341570299, Action ==> 0\n",
      "[-1.07591083 -1.07591083 -1.07591083 -1.07591083]\n",
      "Episode * 5 * Avg Reward is ==> -84.199098848861, Action ==> 0\n",
      "[-1.27439472 -1.27439472 -1.27439472 -1.27439472]\n",
      "Episode * 5 * Avg Reward is ==> -86.50439580662301, Action ==> 0\n",
      "[-1.18456939 -1.18456939 -1.18456939 -1.18456939]\n",
      "Episode * 5 * Avg Reward is ==> -87.58091670198, Action ==> 0\n",
      "[-1.2089391 -1.2089391 -1.2089391 -1.2089391]\n",
      "Episode * 5 * Avg Reward is ==> -88.99784570911, Action ==> 0\n",
      "[-1.08564406 -1.08564406 -1.08564406 -1.08564406]\n",
      "Episode * 5 * Avg Reward is ==> -91.56886498038989, Action ==> 0\n",
      "[-1.15449633 -1.15449633 -1.15449633 -1.15449633]\n",
      "Episode * 5 * Avg Reward is ==> -92.7370509440375, Action ==> 0\n",
      "[-1.37933174 -1.37933174 -1.37933174 -1.37933174]\n",
      "Episode * 5 * Avg Reward is ==> -93.598831120981, Action ==> 0\n",
      "[-1.48818324 -1.48818324 -1.48818324 -1.48818324]\n",
      "Episode * 5 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 5 * Avg Reward is ==> -2012.4600524470404\n",
      "[-1.33029715 -1.33029715 -1.33029715 -1.33029715]\n",
      "Episode * 6 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-1.26996013 -1.26996013 -1.26996013 -1.26996013]\n",
      "Episode * 6 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-1.31180773 -1.31180773 -1.31180773 -1.31180773]\n",
      "Episode * 6 * Avg Reward is ==> -70.63558100536301, Action ==> 0\n",
      "[-1.12872706 -1.12872706 -1.12872706 -1.12872706]\n",
      "Episode * 6 * Avg Reward is ==> -71.120222591422, Action ==> 0\n",
      "[-1.10630325 -1.10630325 -1.10630325 -1.10630325]\n",
      "Episode * 6 * Avg Reward is ==> -71.57996849821299, Action ==> 0\n",
      "[-1.07828452 -1.07828452 -1.07828452 -1.07828452]\n",
      "Episode * 6 * Avg Reward is ==> -72.250861124049, Action ==> 0\n",
      "[-1.08158775 -1.08158775 -1.08158775 -1.08158775]\n",
      "Episode * 6 * Avg Reward is ==> -74.04500060541899, Action ==> 0\n",
      "[-0.86689286 -0.86689286 -0.86689286 -0.86689286]\n",
      "Episode * 6 * Avg Reward is ==> -76.64082308137901, Action ==> 0\n",
      "[-0.9541933 -0.9541933 -0.9541933 -0.9541933]\n",
      "Episode * 6 * Avg Reward is ==> -79.662143242702, Action ==> 0\n",
      "[-1.03870585 -1.03870585 -1.03870585 -1.03870585]\n",
      "Episode * 6 * Avg Reward is ==> -81.705799336225, Action ==> 0\n",
      "[-0.93033119 -0.93033119 -0.93033119 -0.93033119]\n",
      "Episode * 6 * Avg Reward is ==> -83.467783535414, Action ==> 0\n",
      "[-0.97517098 -0.97517098 -0.97517098 -0.97517098]\n",
      "Episode * 6 * Avg Reward is ==> -87.75192611053801, Action ==> 0\n",
      "[-1.03892586 -1.03892586 -1.03892586 -1.03892586]\n",
      "Episode * 6 * Avg Reward is ==> -89.204836552072, Action ==> 0\n",
      "[-0.83452316 -0.83452316 -0.83452316 -0.83452316]\n",
      "Episode * 6 * Avg Reward is ==> -90.9827085328367, Action ==> 0\n",
      "[-0.92223654 -0.92223654 -0.92223654 -0.92223654]\n",
      "Episode * 6 * Avg Reward is ==> -92.5478791595887, Action ==> 0\n",
      "[-0.96944188 -0.96944188 -0.96944188 -0.96944188]\n",
      "Episode * 6 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 6 * Avg Reward is ==> -1886.9079782939232\n",
      "[-0.8563131 -0.8563131 -0.8563131 -0.8563131]\n",
      "Episode * 7 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.92068197 -0.92068197 -0.92068197 -0.92068197]\n",
      "Episode * 7 * Avg Reward is ==> -70.53893241956, Action ==> 0\n",
      "[-0.93129023 -0.93129023 -0.93129023 -0.93129023]\n",
      "Episode * 7 * Avg Reward is ==> -74.148347619122, Action ==> 0\n",
      "[-0.74674387 -0.74674387 -0.74674387 -0.74674387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 7 * Avg Reward is ==> -78.19894962564999, Action ==> 0\n",
      "[-0.70257178 -0.70257178 -0.70257178 -0.70257178]\n",
      "Episode * 7 * Avg Reward is ==> -81.507374472747, Action ==> 0\n",
      "[-0.6879802 -0.6879802 -0.6879802 -0.6879802]\n",
      "Episode * 7 * Avg Reward is ==> -83.111537066329, Action ==> 0\n",
      "[-0.68047203 -0.68047203 -0.68047203 -0.68047203]\n",
      "Episode * 7 * Avg Reward is ==> -84.363310232353, Action ==> 0\n",
      "[-0.63568692 -0.63568692 -0.63568692 -0.63568692]\n",
      "Episode * 7 * Avg Reward is ==> -83.703796795716, Action ==> 0\n",
      "[-0.68758463 -0.68758463 -0.68758463 -0.68758463]\n",
      "Episode * 7 * Avg Reward is ==> -82.07382533012499, Action ==> 0\n",
      "[-0.81883599 -0.81883599 -0.81883599 -0.81883599]\n",
      "Episode * 7 * Avg Reward is ==> -80.312688642723, Action ==> 0\n",
      "[-0.68273939 -0.68273939 -0.68273939 -0.68273939]\n",
      "Episode * 7 * Avg Reward is ==> -81.864049095101, Action ==> 0\n",
      "[-0.58044575 -0.58044575 -0.58044575 -0.58044575]\n",
      "Episode * 7 * Avg Reward is ==> -83.579287432206, Action ==> 0\n",
      "[-0.60824624 -0.60824624 -0.60824624 -0.60824624]\n",
      "Episode * 7 * Avg Reward is ==> -85.18560836339199, Action ==> 0\n",
      "[-0.64488397 -0.64488397 -0.64488397 -0.64488397]\n",
      "Episode * 7 * Avg Reward is ==> -87.135592063203, Action ==> 0\n",
      "[-0.70240216 -0.70240216 -0.70240216 -0.70240216]\n",
      "Episode * 7 * Avg Reward is ==> -88.20070253362, Action ==> 0\n",
      "[-0.61272962 -0.61272962 -0.61272962 -0.61272962]\n",
      "Episode * 7 * Avg Reward is ==> -89.97322935187499, Action ==> 0\n",
      "[-0.45939512 -0.45939512 -0.45939512 -0.45939512]\n",
      "Episode * 7 * Avg Reward is ==> -91.03868320551089, Action ==> 0\n",
      "[-0.48174821 -0.48174821 -0.48174821 -0.48174821]\n",
      "Episode * 7 * Avg Reward is ==> -92.45784431855259, Action ==> 0\n",
      "[-0.46034789 -0.46034789 -0.46034789 -0.46034789]\n",
      "Episode * 7 * Avg Reward is ==> -93.5213818670273, Action ==> 0\n",
      "[-0.3288925 -0.3288925 -0.3288925 -0.3288925]\n",
      "Episode * 7 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 7 * Avg Reward is ==> -1851.9088735615346\n",
      "[-0.24125895 -0.24125895 -0.24125895 -0.24125895]\n",
      "Episode * 8 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.1987722 -0.1987722 -0.1987722 -0.1987722]\n",
      "Episode * 8 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.16464737 -0.16464737 -0.16464737 -0.16464737]\n",
      "Episode * 8 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.27086553 -0.27086553 -0.27086553 -0.27086553]\n",
      "Episode * 8 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.33654594 -0.33654594 -0.33654594 -0.33654594]\n",
      "Episode * 8 * Avg Reward is ==> -70.89468910563099, Action ==> 0\n",
      "[-0.44897626 -0.44897626 -0.44897626 -0.44897626]\n",
      "Episode * 8 * Avg Reward is ==> -73.83846440451, Action ==> 0\n",
      "[-0.38372019 -0.38372019 -0.38372019 -0.38372019]\n",
      "Episode * 8 * Avg Reward is ==> -76.743089644836, Action ==> 0\n",
      "[-0.30369742 -0.30369742 -0.30369742 -0.30369742]\n",
      "Episode * 8 * Avg Reward is ==> -79.44788377358401, Action ==> 0\n",
      "[-0.23197857 -0.23197857 -0.23197857 -0.23197857]\n",
      "Episode * 8 * Avg Reward is ==> -81.73749409605, Action ==> 0\n",
      "[-0.3068708 -0.3068708 -0.3068708 -0.3068708]\n",
      "Episode * 8 * Avg Reward is ==> -87.035544283073, Action ==> 0\n",
      "[-0.49799005 -0.49799005 -0.49799005 -0.49799005]\n",
      "Episode * 8 * Avg Reward is ==> -89.454539708195, Action ==> 0\n",
      "[-0.54859247 -0.54859247 -0.54859247 -0.54859247]\n",
      "Episode * 8 * Avg Reward is ==> -91.5196848012838, Action ==> 0\n",
      "[-0.57680286 -0.57680286 -0.57680286 -0.57680286]\n",
      "Episode * 8 * Avg Reward is ==> -93.2279658184517, Action ==> 0\n",
      "[-0.65383891 -0.65383891 -0.65383891 -0.65383891]\n",
      "Episode * 8 * Avg Reward is ==> -92.7047065111644, Action ==> 0\n",
      "[-0.6845388 -0.6845388 -0.6845388 -0.6845388]\n",
      "Episode * 8 * Avg Reward is ==> -91.208365148795, Action ==> 0\n",
      "[-0.67009259 -0.67009259 -0.67009259 -0.67009259]\n",
      "Episode * 8 * Avg Reward is ==> -90.3419900263938, Action ==> 0\n",
      "[-0.59440667 -0.59440667 -0.59440667 -0.59440667]\n",
      "Episode * 8 * Avg Reward is ==> -90.4136757625827, Action ==> 0\n",
      "[-0.5852066 -0.5852066 -0.5852066 -0.5852066]\n",
      "Episode * 8 * Avg Reward is ==> -92.7030552356827, Action ==> 0\n",
      "[-0.68347619 -0.68347619 -0.68347619 -0.68347619]\n",
      "Episode * 8 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 8 * Avg Reward is ==> -1788.5046818680567\n",
      "[-0.75510783 -0.75510783 -0.75510783 -0.75510783]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.82317562 -0.82317562 -0.82317562 -0.82317562]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.61943723 -0.61943723 -0.61943723 -0.61943723]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.76348303 -0.76348303 -0.76348303 -0.76348303]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.72179315 -0.72179315 -0.72179315 -0.72179315]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.65279766 -0.65279766 -0.65279766 -0.65279766]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.66824423 -0.66824423 -0.66824423 -0.66824423]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.64466046 -0.64466046 -0.64466046 -0.64466046]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.44049181 -0.44049181 -0.44049181 -0.44049181]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.49989015 -0.49989015 -0.49989015 -0.49989015]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.3592546 -0.3592546 -0.3592546 -0.3592546]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.41912299 -0.41912299 -0.41912299 -0.41912299]\n",
      "Episode * 9 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.35968905 -0.35968905 -0.35968905 -0.35968905]\n",
      "Episode * 9 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.47235321 -0.47235321 -0.47235321 -0.47235321]\n",
      "Episode * 9 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.4679333 -0.4679333 -0.4679333 -0.4679333]\n",
      "Episode * 9 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.53799232 -0.53799232 -0.53799232 -0.53799232]\n",
      "Episode * 9 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.50524517 -0.50524517 -0.50524517 -0.50524517]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.59718234 -0.59718234 -0.59718234 -0.59718234]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.63730905 -0.63730905 -0.63730905 -0.63730905]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.61824053 -0.61824053 -0.61824053 -0.61824053]\n",
      "Episode * 9 * Avg Reward is ==> 7, Action ==> 0\n",
      "[-0.70889529 -0.70889529 -0.70889529 -0.70889529]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.78952858 -0.78952858 -0.78952858 -0.78952858]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.80025937 -0.80025937 -0.80025937 -0.80025937]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.77827232 -0.77827232 -0.77827232 -0.77827232]\n",
      "Episode * 9 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.85961751 -0.85961751 -0.85961751 -0.85961751]\n",
      "Episode * 9 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.79552705 -0.79552705 -0.79552705 -0.79552705]\n",
      "Episode * 9 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.76488406 -0.76488406 -0.76488406 -0.76488406]\n",
      "Episode * 9 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.6254831 -0.6254831 -0.6254831 -0.6254831]\n",
      "Episode * 9 * Avg Reward is ==> 5, Action ==> 0\n",
      "[-0.55422792 -0.55422792 -0.55422792 -0.55422792]\n",
      "Episode * 9 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.42340554 -0.42340554 -0.42340554 -0.42340554]\n",
      "Episode * 9 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.42162944 -0.42162944 -0.42162944 -0.42162944]\n",
      "Episode * 9 * Avg Reward is ==> 4, Action ==> 0\n",
      "[-0.43100614 -0.43100614 -0.43100614 -0.43100614]\n",
      "Episode * 9 * Avg Reward is ==> -70.246126288686, Action ==> 0\n",
      "[-0.41696715 -0.41696715 -0.41696715 -0.41696715]\n",
      "Episode * 9 * Avg Reward is ==> -71.13056509179701, Action ==> 0\n",
      "[-0.3274617 -0.3274617 -0.3274617 -0.3274617]\n",
      "Episode * 9 * Avg Reward is ==> -72.01786812824899, Action ==> 0\n",
      "[-0.19236326 -0.19236326 -0.19236326 -0.19236326]\n",
      "Episode * 9 * Avg Reward is ==> -73.083081763618, Action ==> 0\n",
      "[-0.0348105 -0.0348105 -0.0348105 -0.0348105]\n",
      "Episode * 9 * Avg Reward is ==> -74.67883729393, Action ==> 0\n",
      "[0.00462853 0.00462853 0.00462853 0.00462853]\n",
      "Episode * 9 * Avg Reward is ==> -76.274890824774, Action ==> 0\n",
      "[-0.02063784 -0.02063784 -0.02063784 -0.02063784]\n",
      "Episode * 9 * Avg Reward is ==> -78.237822632789, Action ==> 0\n",
      "[0.03026938 0.03026938 0.03026938 0.03026938]\n",
      "Episode * 9 * Avg Reward is ==> -79.11389901712799, Action ==> 0\n",
      "[-0.03199345 -0.03199345 -0.03199345 -0.03199345]\n",
      "Episode * 9 * Avg Reward is ==> -81.475653557468, Action ==> 0\n",
      "[0.10103091 0.10103091 0.10103091 0.10103091]\n",
      "Episode * 9 * Avg Reward is ==> -83.37147881129701, Action ==> 0\n",
      "[0.3339649 0.3339649 0.3339649 0.3339649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 9 * Avg Reward is ==> -84.43708377943399, Action ==> 0\n",
      "[0.28562136 0.28562136 0.28562136 0.28562136]\n",
      "Episode * 9 * Avg Reward is ==> -84.967304438054, Action ==> 0\n",
      "[0.35598444 0.35598444 0.35598444 0.35598444]\n",
      "Episode * 9 * Avg Reward is ==> -85.85511886024, Action ==> 0\n",
      "[0.32781804 0.32781804 0.32781804 0.32781804]\n",
      "Episode * 9 * Avg Reward is ==> -87.875816883675, Action ==> 0\n",
      "[0.28228245 0.28228245 0.28228245 0.28228245]\n",
      "Episode * 9 * Avg Reward is ==> -89.57918400627, Action ==> 0\n",
      "[0.17209239 0.17209239 0.17209239 0.17209239]\n",
      "Episode * 9 * Avg Reward is ==> -91.5320852826735, Action ==> 0\n",
      "[0.04700623 0.04700623 0.04700623 0.04700623]\n",
      "Episode * 9 * Avg Reward is ==> -92.5962934655435, Action ==> 0\n",
      "[0.08703497 0.08703497 0.08703497 0.08703497]\n",
      "Episode * 9 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 9 * Avg Reward is ==> -1737.4015246938136\n",
      "[0.09154265 0.09154265 0.09154265 0.09154265]\n",
      "Episode * 10 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.11192969 0.11192969 0.11192969 0.11192969]\n",
      "Episode * 10 * Avg Reward is ==> 11, Action ==> 0\n",
      "[0.03892885 0.03892885 0.03892885 0.03892885]\n",
      "Episode * 10 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.06867539 0.06867539 0.06867539 0.06867539]\n",
      "Episode * 10 * Avg Reward is ==> 10, Action ==> 0\n",
      "[0.09571288 0.09571288 0.09571288 0.09571288]\n",
      "Episode * 10 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.23099892 0.23099892 0.23099892 0.23099892]\n",
      "Episode * 10 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.35622023 0.35622023 0.35622023 0.35622023]\n",
      "Episode * 10 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.40696911 0.40696911 0.40696911 0.40696911]\n",
      "Episode * 10 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.65745809 0.65745809 0.65745809 0.65745809]\n",
      "Episode * 10 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.66902026 0.66902026 0.66902026 0.66902026]\n",
      "Episode * 10 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.46988056 0.46988056 0.46988056 0.46988056]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.46813355 0.46813355 0.46813355 0.46813355]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.69817135 0.69817135 0.69817135 0.69817135]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.68027528 0.68027528 0.68027528 0.68027528]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.67365629 0.67365629 0.67365629 0.67365629]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.75242783 0.75242783 0.75242783 0.75242783]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.61659703 0.61659703 0.61659703 0.61659703]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.53041279 0.53041279 0.53041279 0.53041279]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.57114576 0.57114576 0.57114576 0.57114576]\n",
      "Episode * 10 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.45219432 0.45219432 0.45219432 0.45219432]\n",
      "Episode * 10 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.4948622 0.4948622 0.4948622 0.4948622]\n",
      "Episode * 10 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.58965841 0.58965841 0.58965841 0.58965841]\n",
      "Episode * 10 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.48612163 0.48612163 0.48612163 0.48612163]\n",
      "Episode * 10 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.59810858 0.59810858 0.59810858 0.59810858]\n",
      "Episode * 10 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.67046556 0.67046556 0.67046556 0.67046556]\n",
      "Episode * 10 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.70559007 0.70559007 0.70559007 0.70559007]\n",
      "Episode * 10 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.83507092 0.83507092 0.83507092 0.83507092]\n",
      "Episode * 10 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.94269302 0.94269302 0.94269302 0.94269302]\n",
      "Episode * 10 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.87684079 0.87684079 0.87684079 0.87684079]\n",
      "Episode * 10 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.84462631 0.84462631 0.84462631 0.84462631]\n",
      "Episode * 10 * Avg Reward is ==> -70.682035732274, Action ==> 0\n",
      "[0.83434824 0.83434824 0.83434824 0.83434824]\n",
      "Episode * 10 * Avg Reward is ==> -72.80466432582, Action ==> 0\n",
      "[0.95267804 0.95267804 0.95267804 0.95267804]\n",
      "Episode * 10 * Avg Reward is ==> -74.76307426943201, Action ==> 0\n",
      "[0.9514484 0.9514484 0.9514484 0.9514484]\n",
      "Episode * 10 * Avg Reward is ==> -76.02508002116099, Action ==> 0\n",
      "[0.87562815 0.87562815 0.87562815 0.87562815]\n",
      "Episode * 10 * Avg Reward is ==> -77.240511512954, Action ==> 0\n",
      "[0.702826 0.702826 0.702826 0.702826]\n",
      "Episode * 10 * Avg Reward is ==> -78.482801820151, Action ==> 0\n",
      "[0.84437991 0.84437991 0.84437991 0.84437991]\n",
      "Episode * 10 * Avg Reward is ==> -80.084689033969, Action ==> 0\n",
      "[0.79211726 0.79211726 0.79211726 0.79211726]\n",
      "Episode * 10 * Avg Reward is ==> -82.985683752036, Action ==> 0\n",
      "[0.87606402 0.87606402 0.87606402 0.87606402]\n",
      "Episode * 10 * Avg Reward is ==> -84.33804526505699, Action ==> 0\n",
      "[0.7521914 0.7521914 0.7521914 0.7521914]\n",
      "Episode * 10 * Avg Reward is ==> -85.40760444851901, Action ==> 0\n",
      "[0.7729331 0.7729331 0.7729331 0.7729331]\n",
      "Episode * 10 * Avg Reward is ==> -86.47413052844499, Action ==> 0\n",
      "[0.79979462 0.79979462 0.79979462 0.79979462]\n",
      "Episode * 10 * Avg Reward is ==> -87.176000300341, Action ==> 0\n",
      "[0.74091257 0.74091257 0.74091257 0.74091257]\n",
      "Episode * 10 * Avg Reward is ==> -89.129737555356, Action ==> 0\n",
      "[0.73676394 0.73676394 0.73676394 0.73676394]\n",
      "Episode * 10 * Avg Reward is ==> -90.01231102937291, Action ==> 0\n",
      "[0.69366858 0.69366858 0.69366858 0.69366858]\n",
      "Episode * 10 * Avg Reward is ==> -91.2560876201931, Action ==> 0\n",
      "[0.74428626 0.74428626 0.74428626 0.74428626]\n",
      "Episode * 10 * Avg Reward is ==> -92.3199258538605, Action ==> 0\n",
      "[0.67051495 0.67051495 0.67051495 0.67051495]\n",
      "Episode * 10 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 10 * Avg Reward is ==> -1689.3816027279163\n",
      "[0.65291951 0.65291951 0.65291951 0.65291951]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.48766569 0.48766569 0.48766569 0.48766569]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.31758536 0.31758536 0.31758536 0.31758536]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.24047748 0.24047748 0.24047748 0.24047748]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.274259 0.274259 0.274259 0.274259]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.55451934 0.55451934 0.55451934 0.55451934]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.36290836 0.36290836 0.36290836 0.36290836]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.323654 0.323654 0.323654 0.323654]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.4038358 0.4038358 0.4038358 0.4038358]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.43398641 0.43398641 0.43398641 0.43398641]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.50693204 0.50693204 0.50693204 0.50693204]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.44856715 0.44856715 0.44856715 0.44856715]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.42700393 0.42700393 0.42700393 0.42700393]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.43452904 0.43452904 0.43452904 0.43452904]\n",
      "Episode * 11 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.43935183 0.43935183 0.43935183 0.43935183]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.45390194 0.45390194 0.45390194 0.45390194]\n",
      "Episode * 11 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.51020143 0.51020143 0.51020143 0.51020143]\n",
      "Episode * 11 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.50014527 0.50014527 0.50014527 0.50014527]\n",
      "Episode * 11 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.42402925 0.42402925 0.42402925 0.42402925]\n",
      "Episode * 11 * Avg Reward is ==> -70.497173897541, Action ==> 0\n",
      "[0.51917244 0.51917244 0.51917244 0.51917244]\n",
      "Episode * 11 * Avg Reward is ==> -71.562353987466, Action ==> 0\n",
      "[0.65757852 0.65757852 0.65757852 0.65757852]\n",
      "Episode * 11 * Avg Reward is ==> -73.160778469832, Action ==> 0\n",
      "[0.69546612 0.69546612 0.69546612 0.69546612]\n",
      "Episode * 11 * Avg Reward is ==> -74.221831668612, Action ==> 0\n",
      "[0.67721394 0.67721394 0.67721394 0.67721394]\n",
      "Episode * 11 * Avg Reward is ==> -75.818466676225, Action ==> 0\n",
      "[0.73809234 0.73809234 0.73809234 0.73809234]\n",
      "Episode * 11 * Avg Reward is ==> -77.239628586681, Action ==> 0\n",
      "[0.78275938 0.78275938 0.78275938 0.78275938]\n",
      "Episode * 11 * Avg Reward is ==> -78.658476463528, Action ==> 0\n",
      "[0.7975568 0.7975568 0.7975568 0.7975568]\n",
      "Episode * 11 * Avg Reward is ==> -79.89898960459, Action ==> 0\n",
      "[0.67328304 0.67328304 0.67328304 0.67328304]\n",
      "Episode * 11 * Avg Reward is ==> -81.498687111127, Action ==> 0\n",
      "[0.6551723 0.6551723 0.6551723 0.6551723]\n",
      "Episode * 11 * Avg Reward is ==> -82.38375115329501, Action ==> 0\n",
      "[0.70234783 0.70234783 0.70234783 0.70234783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 11 * Avg Reward is ==> -84.867433122754, Action ==> 0\n",
      "[0.83386836 0.83386836 0.83386836 0.83386836]\n",
      "Episode * 11 * Avg Reward is ==> -86.11139589423699, Action ==> 0\n",
      "[0.7045731 0.7045731 0.7045731 0.7045731]\n",
      "Episode * 11 * Avg Reward is ==> -88.414330295181, Action ==> 0\n",
      "[0.59248437 0.59248437 0.59248437 0.59248437]\n",
      "Episode * 11 * Avg Reward is ==> -91.2566208266238, Action ==> 0\n",
      "[0.55714815 0.55714815 0.55714815 0.55714815]\n",
      "Episode * 11 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 11 * Avg Reward is ==> -1641.8156289803976\n",
      "[0.58039804 0.58039804 0.58039804 0.58039804]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.59867794 0.59867794 0.59867794 0.59867794]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.50469868 0.50469868 0.50469868 0.50469868]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.59525918 0.59525918 0.59525918 0.59525918]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.61929768 0.61929768 0.61929768 0.61929768]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.59686028 0.59686028 0.59686028 0.59686028]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.68989639 0.68989639 0.68989639 0.68989639]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.7312632 0.7312632 0.7312632 0.7312632]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.71255492 0.71255492 0.71255492 0.71255492]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.80244107 0.80244107 0.80244107 0.80244107]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.92469818 0.92469818 0.92469818 0.92469818]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.87824072 0.87824072 0.87824072 0.87824072]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.73185308 0.73185308 0.73185308 0.73185308]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.89706055 0.89706055 0.89706055 0.89706055]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.85865529 0.85865529 0.85865529 0.85865529]\n",
      "Episode * 12 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.00537737 1.00537737 1.00537737 1.00537737]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.01047879 1.01047879 1.01047879 1.01047879]\n",
      "Episode * 12 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.98104743 0.98104743 0.98104743 0.98104743]\n",
      "Episode * 12 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.86995891 0.86995891 0.86995891 0.86995891]\n",
      "Episode * 12 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.78558846 0.78558846 0.78558846 0.78558846]\n",
      "Episode * 12 * Avg Reward is ==> -72.263009141299, Action ==> 0\n",
      "[0.73131514 0.73131514 0.73131514 0.73131514]\n",
      "Episode * 12 * Avg Reward is ==> -74.746649338686, Action ==> 0\n",
      "[0.80339942 0.80339942 0.80339942 0.80339942]\n",
      "Episode * 12 * Avg Reward is ==> -76.346373139365, Action ==> 0\n",
      "[0.94130039 0.94130039 0.94130039 0.94130039]\n",
      "Episode * 12 * Avg Reward is ==> -78.65406574874, Action ==> 0\n",
      "[1.05227006 1.05227006 1.05227006 1.05227006]\n",
      "Episode * 12 * Avg Reward is ==> -80.245438062668, Action ==> 0\n",
      "[0.88942386 0.88942386 0.88942386 0.88942386]\n",
      "Episode * 12 * Avg Reward is ==> -81.136189896628, Action ==> 0\n",
      "[0.83104859 0.83104859 0.83104859 0.83104859]\n",
      "Episode * 12 * Avg Reward is ==> -81.842081326805, Action ==> 0\n",
      "[0.88414123 0.88414123 0.88414123 0.88414123]\n",
      "Episode * 12 * Avg Reward is ==> -82.728182044828, Action ==> 0\n",
      "[0.79904399 0.79904399 0.79904399 0.79904399]\n",
      "Episode * 12 * Avg Reward is ==> -83.971038792924, Action ==> 0\n",
      "[0.6089244 0.6089244 0.6089244 0.6089244]\n",
      "Episode * 12 * Avg Reward is ==> -87.167822279366, Action ==> 0\n",
      "[0.68031479 0.68031479 0.68031479 0.68031479]\n",
      "Episode * 12 * Avg Reward is ==> -89.29274225999299, Action ==> 0\n",
      "[0.49239958 0.49239958 0.49239958 0.49239958]\n",
      "Episode * 12 * Avg Reward is ==> -90.5327302598361, Action ==> 0\n",
      "[0.41078824 0.41078824 0.41078824 0.41078824]\n",
      "Episode * 12 * Avg Reward is ==> -92.8713955246264, Action ==> 0\n",
      "[0.54335029 0.54335029 0.54335029 0.54335029]\n",
      "Episode * 12 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 12 * Avg Reward is ==> -1597.9680973523489\n",
      "[0.53318191 0.53318191 0.53318191 0.53318191]\n",
      "Episode * 13 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.61428935 0.61428935 0.61428935 0.61428935]\n",
      "Episode * 13 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.60592465 0.60592465 0.60592465 0.60592465]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.81023014 0.81023014 0.81023014 0.81023014]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.98590363 0.98590363 0.98590363 0.98590363]\n",
      "Episode * 13 * Avg Reward is ==> -71.84722234886101, Action ==> 0\n",
      "[0.87907495 0.87907495 0.87907495 0.87907495]\n",
      "Episode * 13 * Avg Reward is ==> -72.69632754538, Action ==> 0\n",
      "[0.8901934 0.8901934 0.8901934 0.8901934]\n",
      "Episode * 13 * Avg Reward is ==> -72.50511154219001, Action ==> 0\n",
      "[0.93409021 0.93409021 0.93409021 0.93409021]\n",
      "Episode * 13 * Avg Reward is ==> -71.366064761629, Action ==> 0\n",
      "[0.81803798 0.81803798 0.81803798 0.81803798]\n",
      "Episode * 13 * Avg Reward is ==> -71.00036568133801, Action ==> 0\n",
      "[0.80585756 0.80585756 0.80585756 0.80585756]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.83861801 0.83861801 0.83861801 0.83861801]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.72049293 0.72049293 0.72049293 0.72049293]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.64132566 0.64132566 0.64132566 0.64132566]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.58180767 0.58180767 0.58180767 0.58180767]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.78325887 0.78325887 0.78325887 0.78325887]\n",
      "Episode * 13 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.79722076 0.79722076 0.79722076 0.79722076]\n",
      "Episode * 13 * Avg Reward is ==> -71.000319779367, Action ==> 0\n",
      "[0.81765779 0.81765779 0.81765779 0.81765779]\n",
      "Episode * 13 * Avg Reward is ==> -72.971627901545, Action ==> 0\n",
      "[0.76667496 0.76667496 0.76667496 0.76667496]\n",
      "Episode * 13 * Avg Reward is ==> -74.746075768801, Action ==> 0\n",
      "[0.88642529 0.88642529 0.88642529 0.88642529]\n",
      "Episode * 13 * Avg Reward is ==> -75.810509795591, Action ==> 0\n",
      "[1.02997419 1.02997419 1.02997419 1.02997419]\n",
      "Episode * 13 * Avg Reward is ==> -78.11680828085599, Action ==> 0\n",
      "[0.98331037 0.98331037 0.98331037 0.98331037]\n",
      "Episode * 13 * Avg Reward is ==> -80.955726333641, Action ==> 0\n",
      "[1.17273813 1.17273813 1.17273813 1.17273813]\n",
      "Episode * 13 * Avg Reward is ==> -83.684711406692, Action ==> 0\n",
      "[1.15120663 1.15120663 1.15120663 1.15120663]\n",
      "Episode * 13 * Avg Reward is ==> -85.74479486911099, Action ==> 0\n",
      "[1.03485741 1.03485741 1.03485741 1.03485741]\n",
      "Episode * 13 * Avg Reward is ==> -87.006234524592, Action ==> 0\n",
      "[1.105948 1.105948 1.105948 1.105948]\n",
      "Episode * 13 * Avg Reward is ==> -89.469030918267, Action ==> 0\n",
      "[1.03824418 1.03824418 1.03824418 1.03824418]\n",
      "Episode * 13 * Avg Reward is ==> -91.6007534093793, Action ==> 0\n",
      "[1.07567856 1.07567856 1.07567856 1.07567856]\n",
      "Episode * 13 * Avg Reward is ==> -93.5511852511825, Action ==> 0\n",
      "[1.00396495 1.00396495 1.00396495 1.00396495]\n",
      "Episode * 13 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 13 * Avg Reward is ==> -1583.9755811213543\n",
      "[1.08472118 1.08472118 1.08472118 1.08472118]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.14629325 1.14629325 1.14629325 1.14629325]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.19058468 1.19058468 1.19058468 1.19058468]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.07697323 1.07697323 1.07697323 1.07697323]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.950844 0.950844 0.950844 0.950844]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.05223237 1.05223237 1.05223237 1.05223237]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.19580854 1.19580854 1.19580854 1.19580854]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.18987992 1.18987992 1.18987992 1.18987992]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.99277064 0.99277064 0.99277064 0.99277064]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.08508919 1.08508919 1.08508919 1.08508919]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.1374735 1.1374735 1.1374735 1.1374735]\n",
      "Episode * 14 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.26725497 1.26725497 1.26725497 1.26725497]\n",
      "Episode * 14 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.25628264 1.25628264 1.25628264 1.25628264]\n",
      "Episode * 14 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.21590561 1.21590561 1.21590561 1.21590561]\n",
      "Episode * 14 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.03911464 1.03911464 1.03911464 1.03911464]\n",
      "Episode * 14 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.09949621 1.09949621 1.09949621 1.09949621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 14 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.0963094 1.0963094 1.0963094 1.0963094]\n",
      "Episode * 14 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.11359775 1.11359775 1.11359775 1.11359775]\n",
      "Episode * 14 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.91943162 0.91943162 0.91943162 0.91943162]\n",
      "Episode * 14 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.96824063 0.96824063 0.96824063 0.96824063]\n",
      "Episode * 14 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.11374023 1.11374023 1.11374023 1.11374023]\n",
      "Episode * 14 * Avg Reward is ==> -70.739063278369, Action ==> 0\n",
      "[0.97390359 0.97390359 0.97390359 0.97390359]\n",
      "Episode * 14 * Avg Reward is ==> -72.33250594059399, Action ==> 0\n",
      "[1.06105502 1.06105502 1.06105502 1.06105502]\n",
      "Episode * 14 * Avg Reward is ==> -74.291023002913, Action ==> 0\n",
      "[1.00027377 1.00027377 1.00027377 1.00027377]\n",
      "Episode * 14 * Avg Reward is ==> -75.171168597822, Action ==> 0\n",
      "[1.14054636 1.14054636 1.14054636 1.14054636]\n",
      "Episode * 14 * Avg Reward is ==> -76.412371004335, Action ==> 0\n",
      "[1.10295117 1.10295117 1.10295117 1.10295117]\n",
      "Episode * 14 * Avg Reward is ==> -78.54170815203, Action ==> 0\n",
      "[1.14935914 1.14935914 1.14935914 1.14935914]\n",
      "Episode * 14 * Avg Reward is ==> -80.31514708944701, Action ==> 0\n",
      "[1.27239686 1.27239686 1.27239686 1.27239686]\n",
      "Episode * 14 * Avg Reward is ==> -81.557242975634, Action ==> 0\n",
      "[1.23739417 1.23739417 1.23739417 1.23739417]\n",
      "Episode * 14 * Avg Reward is ==> -82.977016101803, Action ==> 0\n",
      "[1.18129848 1.18129848 1.18129848 1.18129848]\n",
      "Episode * 14 * Avg Reward is ==> -85.460532849978, Action ==> 0\n",
      "[0.99815532 0.99815532 0.99815532 0.99815532]\n",
      "Episode * 14 * Avg Reward is ==> -86.524542798823, Action ==> 0\n",
      "[1.09918417 1.09918417 1.09918417 1.09918417]\n",
      "Episode * 14 * Avg Reward is ==> -89.36286143977699, Action ==> 0\n",
      "[0.99476628 0.99476628 0.99476628 0.99476628]\n",
      "Episode * 14 * Avg Reward is ==> -90.6056175520057, Action ==> 0\n",
      "[1.1158275 1.1158275 1.1158275 1.1158275]\n",
      "Episode * 14 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 14 * Avg Reward is ==> -1546.3965957654993\n",
      "[1.09320285 1.09320285 1.09320285 1.09320285]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.06482976 1.06482976 1.06482976 1.06482976]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.87422281 0.87422281 0.87422281 0.87422281]\n",
      "Episode * 15 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.94491591 0.94491591 0.94491591 0.94491591]\n",
      "Episode * 15 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.02796378 1.02796378 1.02796378 1.02796378]\n",
      "Episode * 15 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.91179146 0.91179146 0.91179146 0.91179146]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.84643933 0.84643933 0.84643933 0.84643933]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.83894488 0.83894488 0.83894488 0.83894488]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.79378619 0.79378619 0.79378619 0.79378619]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.63164011 0.63164011 0.63164011 0.63164011]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.71445766 0.71445766 0.71445766 0.71445766]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.72164507 0.72164507 0.72164507 0.72164507]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.76000103 0.76000103 0.76000103 0.76000103]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.88948435 0.88948435 0.88948435 0.88948435]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.03386036 1.03386036 1.03386036 1.03386036]\n",
      "Episode * 15 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.77242855 0.77242855 0.77242855 0.77242855]\n",
      "Episode * 15 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.78175605 0.78175605 0.78175605 0.78175605]\n",
      "Episode * 15 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.74093636 0.74093636 0.74093636 0.74093636]\n",
      "Episode * 15 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.73514239 0.73514239 0.73514239 0.73514239]\n",
      "Episode * 15 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.91569028 0.91569028 0.91569028 0.91569028]\n",
      "Episode * 15 * Avg Reward is ==> -70.62203958456699, Action ==> 0\n",
      "[0.94011021 0.94011021 0.94011021 0.94011021]\n",
      "Episode * 15 * Avg Reward is ==> -72.751943712466, Action ==> 0\n",
      "[0.9200923 0.9200923 0.9200923 0.9200923]\n",
      "Episode * 15 * Avg Reward is ==> -74.349943616273, Action ==> 0\n",
      "[1.10692725 1.10692725 1.10692725 1.10692725]\n",
      "Episode * 15 * Avg Reward is ==> -76.657507172282, Action ==> 0\n",
      "[1.11365327 1.11365327 1.11365327 1.11365327]\n",
      "Episode * 15 * Avg Reward is ==> -79.139570305191, Action ==> 0\n",
      "[1.13231072 1.13231072 1.13231072 1.13231072]\n",
      "Episode * 15 * Avg Reward is ==> -81.441228978767, Action ==> 0\n",
      "[1.15070283 1.15070283 1.15070283 1.15070283]\n",
      "Episode * 15 * Avg Reward is ==> -84.81021665977599, Action ==> 0\n",
      "[1.29117769 1.29117769 1.29117769 1.29117769]\n",
      "Episode * 15 * Avg Reward is ==> -85.877662948046, Action ==> 0\n",
      "[1.3337421 1.3337421 1.3337421 1.3337421]\n",
      "Episode * 15 * Avg Reward is ==> -88.714733716862, Action ==> 0\n",
      "[1.30728664 1.30728664 1.30728664 1.30728664]\n",
      "Episode * 15 * Avg Reward is ==> -90.1798150016301, Action ==> 0\n",
      "[1.32457286 1.32457286 1.32457286 1.32457286]\n",
      "Episode * 15 * Avg Reward is ==> -92.973010882238, Action ==> 0\n",
      "[1.44829024 1.44829024 1.44829024 1.44829024]\n",
      "Episode * 15 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 15 * Avg Reward is ==> -1505.5291630662869\n",
      "[1.37508648 1.37508648 1.37508648 1.37508648]\n",
      "Episode * 16 * Avg Reward is ==> 8, Action ==> 0\n",
      "[1.38306748 1.38306748 1.38306748 1.38306748]\n",
      "Episode * 16 * Avg Reward is ==> 8, Action ==> 0\n",
      "[1.30729066 1.30729066 1.30729066 1.30729066]\n",
      "Episode * 16 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.33542201 1.33542201 1.33542201 1.33542201]\n",
      "Episode * 16 * Avg Reward is ==> 7, Action ==> 0\n",
      "[1.26339126 1.26339126 1.26339126 1.26339126]\n",
      "Episode * 16 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.36384518 1.36384518 1.36384518 1.36384518]\n",
      "Episode * 16 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.18895217 1.18895217 1.18895217 1.18895217]\n",
      "Episode * 16 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.24688986 1.24688986 1.24688986 1.24688986]\n",
      "Episode * 16 * Avg Reward is ==> 4, Action ==> 0\n",
      "[1.29331048 1.29331048 1.29331048 1.29331048]\n",
      "Episode * 16 * Avg Reward is ==> -73.406636384393, Action ==> 0\n",
      "[1.15940886 1.15940886 1.15940886 1.15940886]\n",
      "Episode * 16 * Avg Reward is ==> -77.06425007876399, Action ==> 0\n",
      "[1.18669706 1.18669706 1.18669706 1.18669706]\n",
      "Episode * 16 * Avg Reward is ==> -80.435479928787, Action ==> 0\n",
      "[1.2493305 1.2493305 1.2493305 1.2493305]\n",
      "Episode * 16 * Avg Reward is ==> -82.734854392456, Action ==> 0\n",
      "[1.12697297 1.12697297 1.12697297 1.12697297]\n",
      "Episode * 16 * Avg Reward is ==> -84.841032442931, Action ==> 0\n",
      "[1.04230622 1.04230622 1.04230622 1.04230622]\n",
      "Episode * 16 * Avg Reward is ==> -86.240419409029, Action ==> 0\n",
      "[1.0696328 1.0696328 1.0696328 1.0696328]\n",
      "Episode * 16 * Avg Reward is ==> -86.45304873180399, Action ==> 0\n",
      "[1.19160863 1.19160863 1.19160863 1.19160863]\n",
      "Episode * 16 * Avg Reward is ==> -85.54172689221599, Action ==> 0\n",
      "[1.31112692 1.31112692 1.31112692 1.31112692]\n",
      "Episode * 16 * Avg Reward is ==> -85.542431063017, Action ==> 0\n",
      "[1.45503662 1.45503662 1.45503662 1.45503662]\n",
      "Episode * 16 * Avg Reward is ==> -86.79419122179, Action ==> 0\n",
      "[1.40253267 1.40253267 1.40253267 1.40253267]\n",
      "Episode * 16 * Avg Reward is ==> -87.679917428116, Action ==> 0\n",
      "[1.30256875 1.30256875 1.30256875 1.30256875]\n",
      "Episode * 16 * Avg Reward is ==> -88.568265541953, Action ==> 0\n",
      "[1.36843752 1.36843752 1.36843752 1.36843752]\n",
      "Episode * 16 * Avg Reward is ==> -90.16400650641481, Action ==> 0\n",
      "[1.37063616 1.37063616 1.37063616 1.37063616]\n",
      "Episode * 16 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 16 * Avg Reward is ==> -1484.2901687695448\n",
      "[1.35034976 1.35034976 1.35034976 1.35034976]\n",
      "Episode * 17 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.23805701 1.23805701 1.23805701 1.23805701]\n",
      "Episode * 17 * Avg Reward is ==> 6, Action ==> 0\n",
      "[1.18237424 1.18237424 1.18237424 1.18237424]\n",
      "Episode * 17 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.26991225 1.26991225 1.26991225 1.26991225]\n",
      "Episode * 17 * Avg Reward is ==> 5, Action ==> 0\n",
      "[1.04387548 1.04387548 1.04387548 1.04387548]\n",
      "Episode * 17 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.92534699 0.92534699 0.92534699 0.92534699]\n",
      "Episode * 17 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.95287211 0.95287211 0.95287211 0.95287211]\n",
      "Episode * 17 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.89084254 0.89084254 0.89084254 0.89084254]\n",
      "Episode * 17 * Avg Reward is ==> -70.895680774828, Action ==> 0\n",
      "[0.83255041 0.83255041 0.83255041 0.83255041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 17 * Avg Reward is ==> -73.734771228476, Action ==> 0\n",
      "[0.77344156 0.77344156 0.77344156 0.77344156]\n",
      "Episode * 17 * Avg Reward is ==> -75.863501632494, Action ==> 0\n",
      "[0.95499652 0.95499652 0.95499652 0.95499652]\n",
      "Episode * 17 * Avg Reward is ==> -77.65287973404, Action ==> 0\n",
      "[0.87828013 0.87828013 0.87828013 0.87828013]\n",
      "Episode * 17 * Avg Reward is ==> -80.476113254385, Action ==> 0\n",
      "[0.99226467 0.99226467 0.99226467 0.99226467]\n",
      "Episode * 17 * Avg Reward is ==> -83.49046876076001, Action ==> 0\n",
      "[0.81879437 0.81879437 0.81879437 0.81879437]\n",
      "Episode * 17 * Avg Reward is ==> -85.26476534150301, Action ==> 0\n",
      "[0.84796997 0.84796997 0.84796997 0.84796997]\n",
      "Episode * 17 * Avg Reward is ==> -88.116327637332, Action ==> 0\n",
      "[0.8108671 0.8108671 0.8108671 0.8108671]\n",
      "Episode * 17 * Avg Reward is ==> -91.65323036346021, Action ==> 0\n",
      "[0.70643919 0.70643919 0.70643919 0.70643919]\n",
      "Episode * 17 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 17 * Avg Reward is ==> -1445.8378115449746\n",
      "[0.81372516 0.81372516 0.81372516 0.81372516]\n",
      "Episode * 18 * Avg Reward is ==> 8, Action ==> 0\n",
      "[1.0373715 1.0373715 1.0373715 1.0373715]\n",
      "Episode * 18 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.9315023 0.9315023 0.9315023 0.9315023]\n",
      "Episode * 18 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.88037252 0.88037252 0.88037252 0.88037252]\n",
      "Episode * 18 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.74190945 0.74190945 0.74190945 0.74190945]\n",
      "Episode * 18 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.49759772 0.49759772 0.49759772 0.49759772]\n",
      "Episode * 18 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.51197069 0.51197069 0.51197069 0.51197069]\n",
      "Episode * 18 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.4482357 0.4482357 0.4482357 0.4482357]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.21768696 0.21768696 0.21768696 0.21768696]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.19614291 0.19614291 0.19614291 0.19614291]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.09249531 0.09249531 0.09249531 0.09249531]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.18302504 0.18302504 0.18302504 0.18302504]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.28168629 0.28168629 0.28168629 0.28168629]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.35639506 0.35639506 0.35639506 0.35639506]\n",
      "Episode * 18 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.23437436 0.23437436 0.23437436 0.23437436]\n",
      "Episode * 18 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.41463842 0.41463842 0.41463842 0.41463842]\n",
      "Episode * 18 * Avg Reward is ==> -70.31849709454501, Action ==> 0\n",
      "[0.31381695 0.31381695 0.31381695 0.31381695]\n",
      "Episode * 18 * Avg Reward is ==> -72.268728589549, Action ==> 0\n",
      "[0.24596124 0.24596124 0.24596124 0.24596124]\n",
      "Episode * 18 * Avg Reward is ==> -74.218339252195, Action ==> 0\n",
      "[0.21866449 0.21866449 0.21866449 0.21866449]\n",
      "Episode * 18 * Avg Reward is ==> -76.703411727345, Action ==> 0\n",
      "[0.2243461 0.2243461 0.2243461 0.2243461]\n",
      "Episode * 18 * Avg Reward is ==> -78.29945728468701, Action ==> 0\n",
      "[0.09180616 0.09180616 0.09180616 0.09180616]\n",
      "Episode * 18 * Avg Reward is ==> -81.137986894367, Action ==> 0\n",
      "[0.18017296 0.18017296 0.18017296 0.18017296]\n",
      "Episode * 18 * Avg Reward is ==> -83.79824038781301, Action ==> 0\n",
      "[0.14218667 0.14218667 0.14218667 0.14218667]\n",
      "Episode * 18 * Avg Reward is ==> -85.038813645351, Action ==> 0\n",
      "[0.16069789 0.16069789 0.16069789 0.16069789]\n",
      "Episode * 18 * Avg Reward is ==> -86.458192749113, Action ==> 0\n",
      "[0.15903337 0.15903337 0.15903337 0.15903337]\n",
      "Episode * 18 * Avg Reward is ==> -87.702700384624, Action ==> 0\n",
      "[0.30240622 0.30240622 0.30240622 0.30240622]\n",
      "Episode * 18 * Avg Reward is ==> -88.649131520981, Action ==> 0\n",
      "[0.20943565 0.20943565 0.20943565 0.20943565]\n",
      "Episode * 18 * Avg Reward is ==> -91.492060807762, Action ==> 0\n",
      "[0.1227046 0.1227046 0.1227046 0.1227046]\n",
      "Episode * 18 * Avg Reward is ==> -93.1333831477278, Action ==> 0\n",
      "[0.08570708 0.08570708 0.08570708 0.08570708]\n",
      "Episode * 18 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 18 * Avg Reward is ==> -1426.5947132260842\n",
      "[0.11887583 0.11887583 0.11887583 0.11887583]\n",
      "Episode * 19 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.03158662 -0.03158662 -0.03158662 -0.03158662]\n",
      "Episode * 19 * Avg Reward is ==> 6, Action ==> 0\n",
      "[-0.0617315 -0.0617315 -0.0617315 -0.0617315]\n",
      "Episode * 19 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.12792213 0.12792213 0.12792213 0.12792213]\n",
      "Episode * 19 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.15494685 0.15494685 0.15494685 0.15494685]\n",
      "Episode * 19 * Avg Reward is ==> -72.77114901352499, Action ==> 0\n",
      "[0.11146418 0.11146418 0.11146418 0.11146418]\n",
      "Episode * 19 * Avg Reward is ==> -79.918536686192, Action ==> 0\n",
      "[-0.07941755 -0.07941755 -0.07941755 -0.07941755]\n",
      "Episode * 19 * Avg Reward is ==> -84.49214461053201, Action ==> 0\n",
      "[-0.0911132 -0.0911132 -0.0911132 -0.0911132]\n",
      "Episode * 19 * Avg Reward is ==> -86.694815736778, Action ==> 0\n",
      "[0.13196239 0.13196239 0.13196239 0.13196239]\n",
      "Episode * 19 * Avg Reward is ==> -88.85140305135401, Action ==> 0\n",
      "[-0.01085839 -0.01085839 -0.01085839 -0.01085839]\n",
      "Episode * 19 * Avg Reward is ==> -90.4380623490527, Action ==> 0\n",
      "[-0.16324447 -0.16324447 -0.16324447 -0.16324447]\n",
      "Episode * 19 * Avg Reward is ==> -90.1377895516371, Action ==> 0\n",
      "[-0.14197841 -0.14197841 -0.14197841 -0.14197841]\n",
      "Episode * 19 * Avg Reward is ==> -89.50165950184899, Action ==> 0\n",
      "[-0.01618811 -0.01618811 -0.01618811 -0.01618811]\n",
      "Episode * 19 * Avg Reward is ==> -88.764815309388, Action ==> 0\n",
      "[-0.0458711 -0.0458711 -0.0458711 -0.0458711]\n",
      "Episode * 19 * Avg Reward is ==> -90.1159023351261, Action ==> 0\n",
      "[-0.07157497 -0.07157497 -0.07157497 -0.07157497]\n",
      "Episode * 19 * Avg Reward is ==> -92.9651771955888, Action ==> 0\n",
      "[-0.04652025 -0.04652025 -0.04652025 -0.04652025]\n",
      "Episode * 19 * Avg Reward is ==> -100, Action ==> 0\n",
      "Episode * 19 * Avg Reward is ==> -1406.9975503318312\n",
      "[-0.03068317 -0.03068317 -0.03068317 -0.03068317]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.08484541 0.08484541 0.08484541 0.08484541]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.14064105 0.14064105 0.14064105 0.14064105]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.13346464 0.13346464 0.13346464 0.13346464]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.20291515 0.20291515 0.20291515 0.20291515]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.09477312 0.09477312 0.09477312 0.09477312]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.14083771 0.14083771 0.14083771 0.14083771]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.19634215 0.19634215 0.19634215 0.19634215]\n",
      "Episode * 20 * Avg Reward is ==> 13, Action ==> 0\n",
      "[0.17785737 0.17785737 0.17785737 0.17785737]\n",
      "Episode * 20 * Avg Reward is ==> 12, Action ==> 0\n",
      "[0.07915046 0.07915046 0.07915046 0.07915046]\n",
      "Episode * 20 * Avg Reward is ==> 12, Action ==> 0\n",
      "[0.02704415 0.02704415 0.02704415 0.02704415]\n",
      "Episode * 20 * Avg Reward is ==> 12, Action ==> 0\n",
      "[-0.03001625 -0.03001625 -0.03001625 -0.03001625]\n",
      "Episode * 20 * Avg Reward is ==> 11, Action ==> 0\n",
      "[-0.03857977 -0.03857977 -0.03857977 -0.03857977]\n",
      "Episode * 20 * Avg Reward is ==> 11, Action ==> 0\n",
      "[-0.22128908 -0.22128908 -0.22128908 -0.22128908]\n",
      "Episode * 20 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.17862356 -0.17862356 -0.17862356 -0.17862356]\n",
      "Episode * 20 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.20331061 -0.20331061 -0.20331061 -0.20331061]\n",
      "Episode * 20 * Avg Reward is ==> 10, Action ==> 0\n",
      "[-0.07036123 -0.07036123 -0.07036123 -0.07036123]\n",
      "Episode * 20 * Avg Reward is ==> 9, Action ==> 0\n",
      "[-0.00227118 -0.00227118 -0.00227118 -0.00227118]\n",
      "Episode * 20 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.175981 0.175981 0.175981 0.175981]\n",
      "Episode * 20 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.24860613 0.24860613 0.24860613 0.24860613]\n",
      "Episode * 20 * Avg Reward is ==> 9, Action ==> 0\n",
      "[0.06382163 0.06382163 0.06382163 0.06382163]\n",
      "Episode * 20 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.08992289 0.08992289 0.08992289 0.08992289]\n",
      "Episode * 20 * Avg Reward is ==> 8, Action ==> 0\n",
      "[-0.07795362 -0.07795362 -0.07795362 -0.07795362]\n",
      "Episode * 20 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.00096059 0.00096059 0.00096059 0.00096059]\n",
      "Episode * 20 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.02326106 0.02326106 0.02326106 0.02326106]\n",
      "Episode * 20 * Avg Reward is ==> 8, Action ==> 0\n",
      "[0.12193016 0.12193016 0.12193016 0.12193016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 20 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.16681821 0.16681821 0.16681821 0.16681821]\n",
      "Episode * 20 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.08131188 0.08131188 0.08131188 0.08131188]\n",
      "Episode * 20 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.22395075 0.22395075 0.22395075 0.22395075]\n",
      "Episode * 20 * Avg Reward is ==> 7, Action ==> 0\n",
      "[0.18835448 0.18835448 0.18835448 0.18835448]\n",
      "Episode * 20 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.14446211 0.14446211 0.14446211 0.14446211]\n",
      "Episode * 20 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.29531868 0.29531868 0.29531868 0.29531868]\n",
      "Episode * 20 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.30549661 0.30549661 0.30549661 0.30549661]\n",
      "Episode * 20 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.36164573 0.36164573 0.36164573 0.36164573]\n",
      "Episode * 20 * Avg Reward is ==> 6, Action ==> 0\n",
      "[0.55841934 0.55841934 0.55841934 0.55841934]\n",
      "Episode * 20 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.49165065 0.49165065 0.49165065 0.49165065]\n",
      "Episode * 20 * Avg Reward is ==> 5, Action ==> 0\n",
      "[0.39580443 0.39580443 0.39580443 0.39580443]\n",
      "Episode * 20 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.50048228 0.50048228 0.50048228 0.50048228]\n",
      "Episode * 20 * Avg Reward is ==> 4, Action ==> 0\n",
      "[0.53910562 0.53910562 0.53910562 0.53910562]\n",
      "Episode * 20 * Avg Reward is ==> -70.980449485342, Action ==> 0\n",
      "[0.69946991 0.69946991 0.69946991 0.69946991]\n",
      "Episode * 20 * Avg Reward is ==> -72.398863238565, Action ==> 0\n",
      "[0.67276994 0.67276994 0.67276994 0.67276994]\n",
      "Episode * 20 * Avg Reward is ==> -73.953266454013, Action ==> 0\n",
      "[0.60055983 0.60055983 0.60055983 0.60055983]\n",
      "Episode * 20 * Avg Reward is ==> -75.911678666791, Action ==> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: kill: No such process\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ddpg/perseguidor_actor/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ddpg/perseguidor_critic/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ddpg/perseguidor_target_actor/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ddpg/perseguidor_target_critic/assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     start_dqn(params, env, load_model)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddpg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mstart_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 238\u001b[0m, in \u001b[0;36mstart_ddpg\u001b[0;34m(params, env, load_model)\u001b[0m\n\u001b[1;32m    235\u001b[0m agent\u001b[38;5;241m.\u001b[39mrecord((prev_state, action, reward, state))\n\u001b[1;32m    236\u001b[0m episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 238\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m update_target(agent\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mactor_model\u001b[38;5;241m.\u001b[39mvariables, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    240\u001b[0m update_target(agent\u001b[38;5;241m.\u001b[39mtarget_critic\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mcritic_model\u001b[38;5;241m.\u001b[39mvariables, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 184\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(reward_batch, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    182\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/Perseguidor/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = PerseguidorEnv()\n",
    "algorithm = \"ddpg\" # \"dqn | ddpg\"\n",
    "load_model=False\n",
    "hidden_layer_neuron = int(((2/3) * env.state_space.shape[0]) + env.action_space.shape[0])\n",
    "\n",
    "params = dict()\n",
    "params['action_space'] = env.action_space.shape[0]\n",
    "params['state_space'] = env.state_space.shape[0]\n",
    "params['name'] = 'Peserguidor'\n",
    "params['episode'] = 1000\n",
    "params['result'] = []\n",
    "params['epsilon'] = 0 if load_model else 1\n",
    "params['gamma'] = .99\n",
    "params['batch_size'] = 500\n",
    "params['epsilon_min'] = .05\n",
    "params['epsilon_decay'] = .995\n",
    "params['learning_rate'] = 0.00025\n",
    "params['layer_sizes'] = [env.action_space.shape[0], hidden_layer_neuron, hidden_layer_neuron]\n",
    "params['tau'] = 0.5\n",
    "params['std_dev'] = 1\n",
    "params['upper_bound'] = 3\n",
    "params['lower_bound'] = 0\n",
    "\n",
    "print('hidden neurons ({})'.format(hidden_layer_neuron))\n",
    "history = get_hitory_list(algorithm, load_model)\n",
    "if algorithm==\"dqn\":\n",
    "    start_dqn(params, env, load_model)\n",
    "elif algorithm==\"ddpg\":\n",
    "    start_ddpg(params, env, load_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2dc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
